{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \n",
    "        \"run_name\":                     \"CLIP-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    "        \"device_id\":                    1,      # GPU id\n",
    "        \"seed\":                         42,     # Random seed\n",
    "        \n",
    "        \"learning_rate\":                1e-4,\n",
    "        \"batch_size\":                   128,\n",
    "        \"epochs\":                       1,\n",
    "        \"model\":                        \"RN50\",\n",
    "        \n",
    "        \"temperature\":                  0.07,\n",
    "        \n",
    "        \"loss_type\":                    \"anchor+lunif\",   # anchor, anchor+lunif\n",
    "        \n",
    "        \"num_train_samples\":            -1,            # -1 for all\n",
    "        \"num_test_samples\":             -1,            # -1 for all\n",
    "        \"evaluate_every_n_batches\":     200,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lunif_loss(x, t=2):\n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    # Check that the score matrix has the correct shape\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "    return eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_embeddings(text_embeddings, vision_embeddings, \n",
    "                         sample_size=1000, method='pca', \n",
    "                         title=\"Embeddings Visualization\",\n",
    "                         save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes text and vision embeddings in 2D or 3D using PCA.\n",
    "\n",
    "    Args:\n",
    "        text_embeddings (torch.Tensor): \n",
    "            Shape [N, D] containing text embeddings.\n",
    "        vision_embeddings (torch.Tensor):\n",
    "            Shape [N, D] containing vision/image embeddings.\n",
    "        sample_size (int): \n",
    "            If the embeddings contain more than 'sample_size' samples, \n",
    "            randomly pick this many for faster plotting. Set -1 to use all.\n",
    "        method (str): \n",
    "            \"pca\". Currently only \"pca\" is implemented below for simplicity.\n",
    "        title (str): \n",
    "            Title for the plot.\n",
    "        save_path (str, optional): \n",
    "            If provided, saves the plot to this path instead of showing it.\n",
    "    \"\"\"\n",
    "    # Detach from graph and bring to CPU if the tensors require grad\n",
    "    text_np = text_embeddings.detach().cpu().numpy()\n",
    "    vision_np = vision_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Optionally downsample for quicker plotting\n",
    "    if sample_size != -1:\n",
    "        n_text = text_np.shape[0]\n",
    "        n_vision = vision_np.shape[0]\n",
    "\n",
    "        n_samples = min(n_text, n_vision)\n",
    "\n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "            text_np = text_np[indices]\n",
    "            vision_np = vision_np[indices]\n",
    "\n",
    "    # Combine for joint dimensionality reduction\n",
    "    all_data = np.concatenate([text_np, vision_np], axis=0)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=3)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 'pca' is implemented in this example.\")\n",
    "\n",
    "    # Split back into text and vision\n",
    "    text_reduced = reduced[: len(text_np)]\n",
    "    vision_reduced = reduced[len(text_np):]\n",
    "\n",
    "    # Plot 3D visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(text_reduced[:, 0], text_reduced[:, 1], text_reduced[:, 2], \n",
    "               c='red', alpha=0.6, label='Text')\n",
    "    ax.scatter(vision_reduced[:, 0], vision_reduced[:, 1], vision_reduced[:, 2], \n",
    "               c='blue', alpha=0.6, label='Vision')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        wandb.log({title: wandb.Image(plt)})\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, test_loader, device):\n",
    "\n",
    "    # Create model & transforms from scratch (no pretrained weights)\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "        config[\"model\"],\n",
    "        pretrained=None,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Set up training parameters from the config\n",
    "    lr = config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "\n",
    "    # Move the model to multiple GPUs\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    current_batch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, captions_list in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            current_batch += 1\n",
    "            \n",
    "            # Move data to the primary device\n",
    "            images = images.to(device)\n",
    "            captions = captions_list\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer.tokenize(captions)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Encode image and text\n",
    "            image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "            text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "            \n",
    "            \n",
    "            if config[\"loss_type\"] == \"anchor\":\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "            elif config[\"loss_type\"] == \"anchor+lunif\":\n",
    "                lunif_img = lunif_loss(image_embeds)\n",
    "                lunif_txt = lunif_loss(text_embeds)\n",
    "                lunif = (lunif_img + lunif_txt) / 2\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif\n",
    "            elif config[\"loss_type\"] == \"lunif(50batch)+frozen(text_embed)\":\n",
    "                if current_batch <= 50:\n",
    "                    lunif_img = lunif_loss(image_embeds)\n",
    "                    lunif_txt = lunif_loss(text_embeds)\n",
    "                    lunif = (lunif_img + lunif_txt) / 2\n",
    "                    loss = lunif\n",
    "                else: # train on anchor loss with frozen text embeddings\n",
    "                    text_embeds = text_embeds.detach()\n",
    "                    loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "                    \n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if current_batch % 50 == 0:\n",
    "                 visualize_embeddings(text_embeds, \n",
    "                     image_embeds, \n",
    "                     sample_size=1000, \n",
    "                     method='pca', \n",
    "                     title=\"CLIP Embeddings Visualization\",\n",
    "                     save_path=\"embeddings_plot.png\")\n",
    "            \n",
    "            if current_batch % config[\"evaluate_every_n_batches\"] == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "                print(\"Evaluating model...\")\n",
    "                test_results = evaluate_model(model, test_loader, device)\n",
    "                \n",
    "                wandb.log(test_results)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(config):\n",
    "\n",
    "    # Path to train images and annotations\n",
    "    train_image_dir = './data/coco/images/train2017/'                          # Path to train2017 images\n",
    "    train_annotation_file = './data/coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "    # Path to test (val) images and annotations\n",
    "    test_image_dir = './data/coco/images/val2017/'                          # Path to val2017 images\n",
    "    test_annotation_file = './data/coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "    # Define the transform to be applied to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to the model's required input size\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_coco = dset.CocoCaptions(\n",
    "        root=train_image_dir,\n",
    "        annFile=train_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_coco = dset.CocoCaptions(\n",
    "        root=test_image_dir,\n",
    "        annFile=test_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    if config[\"num_train_samples\"] != -1:\n",
    "        print(f\"Subsetting the training dataset to {config['num_train_samples']} samples\")\n",
    "        # Subset the training dataset\n",
    "        num_training_samples = config[\"num_train_samples\"]\n",
    "        subset_indices = list(range(num_training_samples))\n",
    "        train_coco = Subset(train_coco, subset_indices)\n",
    "    \n",
    "    if config[\"num_test_samples\"] != -1:\n",
    "        print(f\"Subsetting the test dataset to {config['num_test_samples']} samples\")\n",
    "        # Subset the test dataset\n",
    "        num_test_samples = config[\"num_test_samples\"]\n",
    "        subset_indices = list(range(num_test_samples))\n",
    "        test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "    # Every image has 5 captions at max, we need to sample one of them\n",
    "    # Create collate function to sample one caption per image\n",
    "    def collate_fn(batch):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        sel_captions = []\n",
    "        for list_captions in captions:\n",
    "            caption = random.choice(list_captions)\n",
    "            sel_captions.append(caption)\n",
    "        return images, sel_captions\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # NumPy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU random numbers\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU random numbers for a single GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random numbers for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark for deterministic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # Finish any existing W&B runs before starting a new one\n",
    "    wandb.finish()\n",
    "\n",
    "    # Initialize your W&B run\n",
    "    wandb.init(project=\"sparsify-clip\", config=config, name=config[\"run_name\"])\n",
    "    \n",
    "    # Print the config\n",
    "    print(\"Config:\", config)\n",
    "    \n",
    "    # Set the device\n",
    "    device_id = config[\"device_id\"]\n",
    "    device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"Loading the dataset...\")\n",
    "    train_loader, test_loader = dataset_loader(config)\n",
    "    print(\"Dataset loaded.\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(config, train_loader, test_loader, device)\n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    print(\"Final evaluation of the model...\")\n",
    "    final_log = evaluate_model(model, test_loader, device)\n",
    "    print(\"Evaluation complete.\\n\")\n",
    "    \n",
    "    # Save the model and upload it to W&B\n",
    "    #torch.save(model.state_dict(), config[\"run_name\"] + \".pt\")\n",
    "    #wandb.save(config[\"run_name\"] + \".pt\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>▁</td></tr><tr><td>backward_ravg</td><td>▁</td></tr><tr><td>forward_r1</td><td>▁</td></tr><tr><td>forward_ravg</td><td>▁</td></tr><tr><td>train_loss</td><td>█▆▅▆▄▃▃▁▁▁▇▇▇▇▆▇▆▆▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>0.034</td></tr><tr><td>backward_ravg</td><td>0.17</td></tr><tr><td>backward_recall</td><td>0.0/0.1/0.34</td></tr><tr><td>forward_r1</td><td>0</td></tr><tr><td>forward_ravg</td><td>0.272</td></tr><tr><td>forward_recall</td><td>0.0/0.2/0.611</td></tr><tr><td>train_loss</td><td>3.21035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP-2025-01-02-16-44-27</strong> at: <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/h0s5jmly' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/h0s5jmly</a><br> View project at: <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250102_164959-h0s5jmly/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250102_165113-dp73zttj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/dp73zttj' target=\"_blank\">CLIP-2025-01-02-16-51-09</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/dp73zttj' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/dp73zttj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'CLIP-2025-01-02-16-51-09', 'device_id': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 1, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor', 'num_train_samples': 30000, 'num_test_samples': 3000, 'evaluate_every_n_batches': 10}\n",
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Subsetting the training dataset to 30000 samples\n",
      "Subsetting the test dataset to 3000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/234 [00:06<02:04,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 10  Loss: 4.85013\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.74it/s]\n",
      "  4%|▍         | 10/234 [00:15<11:43,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.1/0.374', 'forward_ravg': 0.181, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.34', 'backward_ravg': 0.181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 19/234 [00:19<02:10,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 20  Loss: 4.85205\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.77it/s]\n",
      "  9%|▊         | 20/234 [00:28<11:03,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.1/0.272', 'forward_ravg': 0.147, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.34', 'backward_ravg': 0.181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 29/234 [00:32<02:05,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 30  Loss: 4.85204\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.64it/s]\n",
      " 13%|█▎        | 30/234 [00:41<10:56,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.2/0.306', 'forward_ravg': 0.181, 'backward_r1': 0.068, 'backward_recall': '0.1/0.2/0.374', 'backward_ravg': 0.204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 39/234 [00:46<02:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 40  Loss: 4.85198\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.68it/s]\n",
      " 17%|█▋        | 40/234 [00:55<10:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.2/0.34', 'forward_ravg': 0.192, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.374', 'backward_ravg': 0.192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 49/234 [00:59<01:53,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 50  Loss: 4.85181\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.70it/s]\n",
      " 21%|██▏       | 50/234 [01:10<10:53,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.2/0.476', 'forward_ravg': 0.26, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.374', 'backward_ravg': 0.204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 59/234 [01:14<01:51,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 60  Loss: 4.85167\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.50it/s]\n",
      " 26%|██▌       | 60/234 [01:23<09:29,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.2/0.34', 'forward_ravg': 0.192, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.408', 'backward_ravg': 0.204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 69/234 [01:28<01:42,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 70  Loss: 4.85151\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.70it/s]\n",
      " 30%|██▉       | 70/234 [01:37<08:33,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.2/0.34', 'forward_ravg': 0.192, 'backward_r1': 0.034, 'backward_recall': '0.0/0.3/0.408', 'backward_ravg': 0.238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 79/234 [01:41<01:34,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 80  Loss: 4.85134\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.72it/s]\n",
      " 34%|███▍      | 80/234 [01:50<08:07,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.3/0.442', 'forward_ravg': 0.26, 'backward_r1': 0.068, 'backward_recall': '0.1/0.2/0.374', 'backward_ravg': 0.204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 89/234 [01:55<01:29,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 90  Loss: 4.85132\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.48it/s]\n",
      " 38%|███▊      | 90/234 [02:04<07:56,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.2/0.645', 'forward_ravg': 0.317, 'backward_r1': 0.034, 'backward_recall': '0.0/0.3/0.408', 'backward_ravg': 0.238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 99/234 [02:08<01:24,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 100  Loss: 4.84986\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.72it/s]\n",
      " 43%|████▎     | 100/234 [02:18<07:31,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.136, 'forward_recall': '0.1/0.3/0.51', 'forward_ravg': 0.328, 'backward_r1': 0.068, 'backward_recall': '0.1/0.2/0.442', 'backward_ravg': 0.226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 109/234 [02:23<01:18,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 110  Loss: 4.83614\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.71it/s]\n",
      " 47%|████▋     | 110/234 [02:32<06:27,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.3/0.645', 'forward_ravg': 0.34, 'backward_r1': 0.0, 'backward_recall': '0.0/0.3/0.442', 'backward_ravg': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 119/234 [02:36<01:10,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 120  Loss: 4.84486\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.73it/s]\n",
      " 51%|█████▏    | 120/234 [02:45<05:53,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.136, 'forward_recall': '0.1/0.4/0.747', 'forward_ravg': 0.419, 'backward_r1': 0.068, 'backward_recall': '0.1/0.3/0.51', 'backward_ravg': 0.283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 129/234 [02:49<01:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 130  Loss: 4.85269\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.65it/s]\n",
      " 56%|█████▌    | 130/234 [02:58<05:29,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.2/0.34', 'forward_ravg': 0.181, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.34', 'backward_ravg': 0.181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 139/234 [03:03<00:58,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 140  Loss: 4.85047\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.73it/s]\n",
      " 60%|█████▉    | 140/234 [03:12<04:55,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.2/0.34', 'forward_ravg': 0.181, 'backward_r1': 0.034, 'backward_recall': '0.0/0.2/0.34', 'backward_ravg': 0.181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 149/234 [03:16<00:52,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 150  Loss: 4.85056\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\n",
      " 64%|██████▍   | 150/234 [03:26<04:41,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.2/0.408', 'forward_ravg': 0.238, 'backward_r1': 0.034, 'backward_recall': '0.0/0.1/0.34', 'backward_ravg': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 159/234 [03:30<00:46,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 160  Loss: 4.85086\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.72it/s]\n",
      " 68%|██████▊   | 160/234 [03:39<03:50,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.3/0.611', 'forward_ravg': 0.317, 'backward_r1': 0.034, 'backward_recall': '0.0/0.1/0.408', 'backward_ravg': 0.192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 169/234 [03:44<00:40,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 170  Loss: 4.84861\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.79it/s]\n",
      " 73%|███████▎  | 170/234 [03:52<03:19,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.034, 'forward_recall': '0.0/0.2/0.34', 'forward_ravg': 0.192, 'backward_r1': 0.068, 'backward_recall': '0.1/0.2/0.306', 'backward_ravg': 0.181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 179/234 [03:57<00:33,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 180  Loss: 4.84469\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:06<00:00,  3.70it/s]\n",
      " 77%|███████▋  | 180/234 [04:06<02:48,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.068, 'forward_recall': '0.1/0.3/0.442', 'forward_ravg': 0.272, 'backward_r1': 0.102, 'backward_recall': '0.1/0.4/0.51', 'backward_ravg': 0.328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 189/234 [04:10<00:27,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 190  Loss: 4.83611\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/23 [00:01<?, ?it/s]\n",
      " 81%|████████  | 189/234 [04:12<01:00,  1.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Baseline\u001b[39;00m\n\u001b[1;32m     10\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with different loss functions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Anchor + Lunif\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m dataset_loader(config)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Final evaluation of the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m final_log \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[22], line 87\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m         test_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog(test_results)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# No gradient needed during evaluation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, captions_list \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Move images to device\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# Tokenize captions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sparsify_clip/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%prun\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    config[\"num_train_samples\"] = 30000\n",
    "    config[\"num_test_samples\"] = 3000\n",
    "    config[\"evaluate_every_n_batches\"] = 10\n",
    "    \n",
    "    # Baseline\n",
    "    config[\"loss_type\"] = \"anchor\"\n",
    "    print(\"\\nTraining Baseline model\")\n",
    "    main(config)\n",
    "    \n",
    "    \n",
    "    # Anchor + Lunif\n",
    "    config[\"loss_type\"] = \"anchor+lunif\"\n",
    "    print(\"\\nTraining Anchor + Lunif model\")\n",
    "    main(config)\n",
    "    \n",
    "    # Lunif(200itr)+frozen(text_embed)\n",
    "    config[\"loss_type\"] = \"lunif(200itr)+frozen(text_embed)\"\n",
    "    print(\"\\nTraining Lunif(200itr)+frozen(text_embed) model\")\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
