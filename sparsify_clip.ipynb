{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \n",
    "        \"run_name\":                     \"CLIP-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    "        \"device_id\":                    1,  # GPU id\n",
    "        \n",
    "        \"learning_rate\":                1e-4,\n",
    "        \"batch_size\":                   128,\n",
    "        \"epochs\":                       1,\n",
    "        \"model\":                        \"RN50\",\n",
    "        \n",
    "        \"temperature\":                  0.07,\n",
    "        \n",
    "        \"loss_type\":                    \"anchor+lunif\",\n",
    "        \n",
    "        \"num_train_samples\":            -1,            # -1 for all\n",
    "        \"num_test_samples\":             -1,            # -1 for all\n",
    "        \"evaluate_every_n_batches\":     50,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings (optional, but typical in CLIP-like models)\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lunif_loss(x, t=2):\n",
    "    \n",
    "    x = F.normalize(x, dim=-1)\n",
    "        \n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get batch sizes\n",
    "    batch_size1 = text_embeddings.shape[0]   # For text embeddings\n",
    "    batch_size2 = visual_embeddings.shape[0]  # For visual embeddings\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    print(len(ids_txt),len(ids))\n",
    "    print(score_matrix.shape)\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "\n",
    "    return eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D  \u001b[38;5;66;03m# In case you want a 3D version\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_embeddings\u001b[39m(text_embeddings, vision_embeddings, \n\u001b[1;32m      8\u001b[0m                          sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m                          title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings Visualization\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m                          save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Visualizes text and vision embeddings in 2D using PCA or (optionally) t-SNE.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m            If provided, saves the plot to this path instead of showing it.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # In case you want a 3D version\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def visualize_embeddings(text_embeddings, vision_embeddings, \n",
    "                         sample_size=1000, method='pca', \n",
    "                         title=\"Embeddings Visualization\",\n",
    "                         save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes text and vision embeddings in 2D using PCA or (optionally) t-SNE.\n",
    "\n",
    "    Args:\n",
    "        text_embeddings (torch.Tensor): \n",
    "            Shape [N, D] containing text embeddings.\n",
    "        vision_embeddings (torch.Tensor):\n",
    "            Shape [N, D] containing vision/image embeddings.\n",
    "        sample_size (int): \n",
    "            If the embeddings contain more than 'sample_size' samples, \n",
    "            randomly pick this many for faster plotting. Set -1 to use all.\n",
    "        method (str): \n",
    "            \"pca\" or \"tsne\". Currently only \"pca\" is implemented below for simplicity.\n",
    "        title (str): \n",
    "            Title for the plot.\n",
    "        save_path (str, optional): \n",
    "            If provided, saves the plot to this path instead of showing it.\n",
    "    \"\"\"\n",
    "    # Detach from graph and bring to CPU if the tensors require grad\n",
    "    text_np = text_embeddings.detach().cpu().numpy()\n",
    "    vision_np = vision_embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    # Optionally downsample for quicker plotting\n",
    "    if sample_size != -1:\n",
    "        n_text = text_np.shape[0]\n",
    "        n_vision = vision_np.shape[0]\n",
    "        \n",
    "        # We assume text_np and vision_np have the same length. If not, adjust accordingly.\n",
    "        # Use the minimum just in case.\n",
    "        n_samples = min(n_text, n_vision)\n",
    "        \n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "            text_np = text_np[indices]\n",
    "            vision_np = vision_np[indices]\n",
    "    \n",
    "    # Combine for joint dimensionality reduction\n",
    "    all_data = np.concatenate([text_np, vision_np], axis=0)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    # Currently implementing PCA; you can implement t-SNE if you prefer\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=2)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 'pca' is implemented in this example.\")\n",
    "    \n",
    "    # Split back into text and vision\n",
    "    text_reduced = reduced[: len(text_np)]\n",
    "    vision_reduced = reduced[len(text_np) : ]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(text_reduced[:, 0], text_reduced[:, 1], c='red', alpha=0.6, label='Text')\n",
    "    plt.scatter(vision_reduced[:, 0], vision_reduced[:, 1], c='blue', alpha=0.6, label='Vision')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, test_loader, device):\n",
    "    \n",
    "    model_name = config[\"model\"]   \n",
    "\n",
    "\n",
    "    # Create model & transforms from scratch (no pretrained weights)\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "        model_name,\n",
    "        pretrained=None,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    # Example config\n",
    "    lr = config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "\n",
    "    # Move the model to multiple GPUs\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    current_batch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, captions_list in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            current_batch += 1\n",
    "            \n",
    "            # Move data to the primary device\n",
    "            images = images.to(device)\n",
    "            captions = captions_list\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer.tokenize(captions)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Encode image and text\n",
    "            image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "            \n",
    "            visualize_embeddings(text_embeds, \n",
    "                     vision_embeds, \n",
    "                     sample_size=1000, \n",
    "                     method='pca', \n",
    "                     title=\"CLIP Embeddings Visualization\",\n",
    "                     save_path=\"embeddings_plot.png\")\n",
    "            \n",
    "            if config[\"loss_type\"] == \"anchor\":\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "            elif config[\"loss_type\"] == \"anchor+lunif\":\n",
    "                lunif_img = lunif_loss(image_embeds)\n",
    "                lunif_txt = lunif_loss(text_embeds)\n",
    "                lunif = (lunif_img + lunif_txt) / 2\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif / 2\n",
    "                \n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if current_batch % config[\"evaluate_every_n_batches\"] == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "                print(\"Evaluating model...\")\n",
    "                test_results = evaluate_model(model, test_loader, device)\n",
    "                \n",
    "                wandb.log(test_results)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(config):\n",
    "\n",
    "    # Path to train images and annotations\n",
    "    train_image_dir = './coco/images/train2017/'                          # Path to train2017 images\n",
    "    train_annotation_file = './coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "    # Path to test (val) images and annotations\n",
    "    test_image_dir = './coco/images/val2017/'                          # Path to val2017 images\n",
    "    test_annotation_file = './coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "    # Define the transform to be applied to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # or whatever size your model expects\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_coco = dset.CocoCaptions(\n",
    "        root=train_image_dir,\n",
    "        annFile=train_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_coco = dset.CocoCaptions(\n",
    "        root=test_image_dir,\n",
    "        annFile=test_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    if config[\"num_train_samples\"] != -1:\n",
    "        print(f\"Subsetting the training dataset to {config['num_train_samples']} samples\")\n",
    "\n",
    "        # Subset the training dataset\n",
    "        num_training_samples = config[\"num_train_samples\"]\n",
    "        subset_indices = list(range(num_training_samples))\n",
    "        train_coco = Subset(train_coco, subset_indices)\n",
    "    \n",
    "    if config[\"num_test_samples\"] != -1:\n",
    "        print(f\"Subsetting the test dataset to {config['num_test_samples']} samples\")\n",
    "\n",
    "        # Subset the test dataset\n",
    "        num_test_samples = config[\"num_test_samples\"]\n",
    "        subset_indices = list(range(num_test_samples))\n",
    "        test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "    # Every image has 5 captions at max, we need to sample one of them\n",
    "    # Create collate function to sample one caption per image\n",
    "    def collate_fn(batch):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        sel_captions = []\n",
    "        for list_captions in captions:\n",
    "            caption = random.choice(list_captions)\n",
    "            sel_captions.append(caption)\n",
    "        return images, sel_captions\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # NumPy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU random numbers\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU random numbers for a single GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random numbers for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark for deterministic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoostale\u001b[0m (\u001b[33mnoostale-organization\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250101_223649-8qs9glle</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/8qs9glle' target=\"_blank\">CLIP-2025-01-01-22-36-48</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/8qs9glle' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/8qs9glle</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'CLIP-2025-01-01-22-36-48', 'device_id': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 1, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor+lunif', 'num_train_samples': -1, 'num_test_samples': -1, 'evaluate_every_n_batches': 50}\n",
      "loading annotations into memory...\n",
      "Done (t=0.64s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 49/924 [00:27<07:40,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 50  Loss: 2.97053\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/924 [00:41<1:06:59,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.12, 'forward_recall': '0.1/0.4/0.921', 'forward_ravg': 0.487, 'backward_r1': 0.14, 'backward_recall': '0.1/0.7/1.082', 'backward_ravg': 0.628}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 99/924 [01:06<07:01,  1.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 100  Loss: 2.65684\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 100/924 [01:20<1:04:19,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.26, 'forward_recall': '0.3/1.0/1.963', 'forward_ravg': 1.075, 'backward_r1': 0.16, 'backward_recall': '0.2/0.7/1.342', 'backward_ravg': 0.741}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 149/924 [01:45<06:48,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 150  Loss: 2.52721\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 150/924 [02:00<59:55,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.22, 'forward_recall': '0.2/1.4/2.384', 'forward_ravg': 1.329, 'backward_r1': 0.341, 'backward_recall': '0.3/1.4/2.704', 'backward_ravg': 1.476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 199/924 [02:24<06:13,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 200  Loss: 2.47699\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 200/924 [02:39<56:52,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.521, 'forward_recall': '0.5/1.9/3.405', 'forward_ravg': 1.956, 'backward_r1': 0.3, 'backward_recall': '0.3/1.4/2.704', 'backward_ravg': 1.469}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 249/924 [03:04<05:50,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 250  Loss: 2.02027\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 250/924 [03:19<52:45,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.521, 'forward_recall': '0.5/2.1/3.646', 'forward_ravg': 2.097, 'backward_r1': 0.561, 'backward_recall': '0.6/1.7/3.045', 'backward_ravg': 1.756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 299/924 [03:44<05:24,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 300  Loss: 2.05095\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 300/924 [03:58<48:26,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.661, 'forward_recall': '0.7/2.6/4.627', 'forward_ravg': 2.624, 'backward_r1': 0.501, 'backward_recall': '0.5/2.4/4.367', 'backward_ravg': 2.411}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 349/924 [04:23<04:57,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 350  Loss: 2.01733\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 350/924 [04:37<44:22,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.521, 'forward_recall': '0.5/2.8/4.888', 'forward_ravg': 2.751, 'backward_r1': 0.601, 'backward_recall': '0.6/2.5/4.708', 'backward_ravg': 2.611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 399/924 [05:02<04:33,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 400  Loss: 1.77863\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 400/924 [05:17<41:21,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.561, 'forward_recall': '0.6/2.8/5.389', 'forward_ravg': 2.918, 'backward_r1': 0.741, 'backward_recall': '0.7/3.0/5.349', 'backward_ravg': 3.025}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 449/924 [05:42<04:06,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 450  Loss: 1.92751\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 450/924 [05:57<37:34,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.721, 'forward_recall': '0.7/3.5/5.97', 'forward_ravg': 3.392, 'backward_r1': 0.741, 'backward_recall': '0.7/2.9/5.228', 'backward_ravg': 2.965}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 499/924 [06:22<03:41,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 500  Loss: 1.96207\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 500/924 [06:36<33:10,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.861, 'forward_recall': '0.9/3.6/6.591', 'forward_ravg': 3.679, 'backward_r1': 0.761, 'backward_recall': '0.8/3.2/5.849', 'backward_ravg': 3.272}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 549/924 [07:01<03:15,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 550  Loss: 1.76249\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 550/924 [07:16<29:33,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.801, 'forward_recall': '0.8/3.4/6.21', 'forward_ravg': 3.472, 'backward_r1': 0.861, 'backward_recall': '0.9/3.2/5.609', 'backward_ravg': 3.239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 599/924 [07:41<02:48,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 600  Loss: 1.68925\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 600/924 [07:55<24:40,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.042, 'forward_recall': '1.0/4.3/7.352', 'forward_ravg': 4.24, 'backward_r1': 1.062, 'backward_recall': '1.1/4.0/7.011', 'backward_ravg': 4.026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 649/924 [08:20<02:21,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 650  Loss: 1.57810\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 650/924 [08:34<21:14,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.262, 'forward_recall': '1.3/4.5/7.772', 'forward_ravg': 4.507, 'backward_r1': 1.182, 'backward_recall': '1.2/4.4/7.412', 'backward_ravg': 4.347}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 699/924 [08:59<01:56,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 700  Loss: 1.56632\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 700/924 [09:13<17:06,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.162, 'forward_recall': '1.2/4.7/7.752', 'forward_ravg': 4.534, 'backward_r1': 1.102, 'backward_recall': '1.1/3.6/6.951', 'backward_ravg': 3.873}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 749/924 [09:38<01:30,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 750  Loss: 1.35949\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 750/924 [09:53<13:33,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.262, 'forward_recall': '1.3/5.0/8.754', 'forward_ravg': 5.015, 'backward_r1': 1.122, 'backward_recall': '1.1/4.6/8.474', 'backward_ravg': 4.728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 799/924 [10:18<01:04,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 800  Loss: 1.28336\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 800/924 [10:32<09:48,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.402, 'forward_recall': '1.4/5.3/9.315', 'forward_ravg': 5.355, 'backward_r1': 1.262, 'backward_recall': '1.3/5.0/8.373', 'backward_ravg': 4.868}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 849/924 [10:58<00:38,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 850  Loss: 1.39391\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 850/924 [11:12<05:47,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.382, 'forward_recall': '1.4/5.3/9.555', 'forward_ravg': 5.422, 'backward_r1': 1.302, 'backward_recall': '1.3/5.0/8.694', 'backward_ravg': 5.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 899/924 [11:37<00:12,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 900  Loss: 1.47090\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:08<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 900/924 [11:51<01:51,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.242, 'forward_recall': '1.2/5.4/9.515', 'forward_ravg': 5.402, 'backward_r1': 1.522, 'backward_recall': '1.5/5.7/9.696', 'backward_ravg': 5.656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [12:04<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Loss: 1.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "Evaluation Results: {'forward_r1': 1.502, 'forward_recall': '1.5/6.3/10.156', 'forward_ravg': 5.996, 'backward_r1': 1.482, 'backward_recall': '1.5/5.5/9.696', 'backward_ravg': 5.556}\n"
     ]
    }
   ],
   "source": [
    "# %%prun\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    set_seed(42)\n",
    "    \n",
    "    # Finish any existing W&B runs before starting a new one\n",
    "    wandb.finish()\n",
    "\n",
    "    # Initialize your W&B run\n",
    "    wandb.init(project=\"sparsify-clip\", config=config, name=config[\"run_name\"])\n",
    "    \n",
    "    # Print the config\n",
    "    print(\"Config:\", config)\n",
    "    \n",
    "    # Set the device\n",
    "    device_id = config[\"device_id\"]\n",
    "    device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the dataset\n",
    "    train_loader, test_loader = dataset_loader(config)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(config, train_loader, test_loader, device)\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    final_log = evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %doctest_mode\\n\\n\\ndef dataset_details():\\n    # Print dataset details\\n    print(\\'Number of samples:\\', len(train_coco)) # 118287 images\\n\\n    # Access a specific sample (4th sample here)\\n    img, target = train_coco[3]  # Load the 4th sample (index 3)\\n\\n    # Display information about the sample\\n    print(\"Image Size:\", img.size())  # Torch tensor size\\n    #plt.imshow(img.permute(1, 2, 0))  # Display the image\\n    print(\"Captions:\", target)  # Captions for the image\\n\\nfor images, captions_list in train_loader:\\n    # images.shape is e.g. (N, 3, 224, 224)\\n    # captions_list has length N, but each item might be a tuple of possible captions\\n\\n    plt.imshow(images[0].permute(1, 2, 0))\\n    plt.show()\\n    plt.imshow(images[1].permute(1, 2, 0))\\n    plt.show()\\n\\n    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\\n    print(\"Captions list length:\", len(captions_list))\\n    \\n    print(\"Captions list:\", list(captions_list))\\n\\n    print(\"Number of chosen captions:\", len(list(captions_list[0])))\\n    \\n    captions = list(captions_list[0])\\n\\n    # Then tokenize\\n    text_tokens = tokenizer.tokenize(captions)\\n    print(\"Text tokens shape:\", text_tokens.shape)\\n\\n    # Now encode\\n    #image_embeds = model.encode_image(images.to(device))\\n    #text_embeds = model.encode_text(text_tokens.to(device))\\n\\n    # Should both be shape (N, D)\\n    #print(\"Image embeds shape:\", image_embeds.shape)\\n    #print(\"Text  embeds shape:\", text_embeds.shape)\\n\\n    break  # just to test one batch\\n    \\n\\ndef collate_fn_debug(batch):\\n    print(\"Bath type:\", type(batch)) # This is a list\\n    print(\"Batch size:\", len(batch))\\n    print(\"Batch:\", batch)\\n    images, captions = zip(*batch)\\n    \\n    print(\"Images type:\", type(images))\\n    print(\"Images size:\", len(images))\\n    print(\"Images:\", images)\\n    \\n    print(\"Captions type:\", type(captions))\\n    print(\"Captions size:\", len(captions))\\n    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\\n    \\n    # Select one caption per image\\n    sel_captions = []\\n    for list_captions in captions:\\n        #print(\"List Captions:\", list_captions)\\n        caption = random.choice(list_captions)\\n        sel_captions.append(caption)\\n    \\n    print(\"Selected Captions:\", sel_captions)    \\n\\n\\n\\nfor images, captions_list in train_loader:\\n    break\\n\\n# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\\n\\n '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
