{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \n",
    "        \"run_name\":                     \"CLIP-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    "        \"device_id\":                    1,      # GPU id\n",
    "        \"seed\":                         42,     # Random seed\n",
    "        \n",
    "        \"learning_rate\":                1e-4,\n",
    "        \"batch_size\":                   128,\n",
    "        \"epochs\":                       1,\n",
    "        \"model\":                        \"RN50\",\n",
    "        \n",
    "        \"temperature\":                  0.07,\n",
    "        \n",
    "        \"loss_type\":                    \"anchor+lunif\",   # anchor, anchor+lunif\n",
    "        \n",
    "        \"num_train_samples\":            -1,            # -1 for all\n",
    "        \"num_test_samples\":             -1,            # -1 for all\n",
    "        \"evaluate_every_n_batches\":     200,\n",
    "        \"visualize_every_n_batches\":    10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lunif_loss(x, t=2):\n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\\n    \\n    # Check that the score matrix has the correct shape\\n    assert score_matrix.shape == (len(ids_txt),len(ids))\\n\\n    if direction == 'forward': ### text-to-vision retrieval\\n        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\\n        rank = []\\n        for i in range(len(ids_txt)):\\n            # gt_indice = ids.index(ids_txt[i][0])\\n            gt_indice = ids.index(ids_txt[i])\\n            rank.append(indice_matrix[i].index(gt_indice))\\n        \\n        rank = torch.tensor(rank).to(score_matrix)\\n        \\n        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\\n        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\\n        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\\n        v_medianR = torch.median(rank).item() +1\\n        v_meanR = torch.mean(rank).item() +1\\n \\n        eval_log = {'forward_r1': round(vr_r1*100,3),\\n                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\\n                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\\n                   }\\n   \\n    else: ### vision-to-text retrieval\\n       \\n        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\\n        rank = []\\n        for i in range(len(ids)):\\n            gt_indices=[]\\n            for idx, id in enumerate(ids_txt):\\n                if id == ids[i]:\\n                    gt_indices.append(idx)\\n\\n            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\\n        \\n        rank = torch.tensor(rank).to(score_matrix)\\n        \\n        tr_r1 = (rank < 1).sum().item() / len(ids)\\n        tr_r5 = (rank < 5).sum().item() / len(ids)\\n        tr_r10 = (rank < 10).sum().item() / len(ids)\\n        t_medianR = torch.median(rank).item() +1\\n        t_meanR = torch.mean(rank).item() +1\\n\\n        eval_log = {\\n                    'backward_r1': round(tr_r1*100,3),\\n                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\\n                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\\n                  }\\n    \\n    return eval_log\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    # Check that the score matrix has the correct shape\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "    return eval_log\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def evaluate_model(model, test_loader, device):\\n    \\'\\'\\'\\n    Evaluate the (OpenCLIP) model on the given test_loader by computing\\n    text-to-image and image-to-text retrieval metrics.\\n\\n    Args:\\n        model (nn.Module): The trained (DataParallel) model.\\n        test_loader (DataLoader): A DataLoader for the evaluation set.\\n        device (torch.device): The device (CPU or GPU).\\n    \\'\\'\\'\\n    \\n    # Put model into eval mode\\n    model.eval()\\n    \\n    # Prepare storage\\n    all_image_embeds = []\\n    all_text_embeds  = []\\n    \\n    # IDs for retrieval\\n    # We\\'ll assign each sample a unique ID. Because your `collate_fn` is\\n    # picking exactly one caption per image, we can treat each batch entry\\n    # as a 1:1 mapping of (image_i <-> text_i).\\n    ids_img = []\\n    ids_txt = []\\n    \\n    current_index = 0\\n\\n    # No gradient needed during evaluation\\n    with torch.no_grad():\\n        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\\n            # Move images to device\\n            images = images.to(device)\\n\\n            # Tokenize captions\\n            text_tokens = tokenizer.tokenize(captions_list)\\n            text_tokens = text_tokens.to(device)\\n\\n            # Extract embeddings using the .module references in DataParallel\\n            image_embeds = model.module.encode_image(images)\\n            text_embeds  = model.module.encode_text(text_tokens)\\n\\n            # Move them to CPU for later concatenation\\n            image_embeds = image_embeds.cpu()\\n            text_embeds  = text_embeds.cpu()\\n            \\n            # Track\\n            bs = images.size(0)\\n            all_image_embeds.append(image_embeds)\\n            all_text_embeds.append(text_embeds)\\n\\n            # For retrieval, we label these samples from current_index to current_index + bs - 1\\n            sample_ids = list(range(current_index, current_index + bs))\\n            ids_img.extend(sample_ids)\\n            ids_txt.extend(sample_ids)\\n            current_index += bs\\n    \\n    # Concatenate everything\\n    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\\n    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\\n\\n    # Normalize embeddings for more stable retrieval\\n    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\\n    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\\n\\n    # Compute pairwise similarity: [N_text, N_image]\\n    # Because we aligned IDs, this is effectively [N, N].\\n    similarity_matrix = all_text_embeds @ all_image_embeds.t()\\n\\n    # Use the given function compute_metric_ret to compute retrieval metrics.\\n    # text->image: direction=\\'forward\\'\\n    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction=\\'forward\\')\\n    # image->text: direction=\\'backward\\'\\n    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction=\\'backward\\')\\n\\n    # You can combine or print them:\\n    final_log = {**log_forward, **log_backward}\\n    print(\"Evaluation Results:\", final_log)\\n\\n    return final_log'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def evaluate_model(model, test_loader, device):\n",
    "    '''\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    '''\n",
    "    \n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "\n",
    "def compute_metric_ret(score_matrix: torch.Tensor, ids: List[int], ids_txt: List[int], direction: str = 'forward') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics for either text-to-vision or vision-to-text retrieval.\n",
    "\n",
    "    Args:\n",
    "        score_matrix (torch.Tensor): Similarity matrix of shape [N_text, N_image].\n",
    "        ids (List[int]): List of image IDs.\n",
    "        ids_txt (List[int]): List of text IDs corresponding to images.\n",
    "        direction (str): 'forward' for text-to-vision, 'backward' for vision-to-text.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing retrieval metrics.\n",
    "    \"\"\"\n",
    "    assert score_matrix.shape == (len(ids_txt), len(ids)), f\"Score matrix shape {score_matrix.shape} does not match (len(ids_txt), len(ids))\"\n",
    "\n",
    "    if direction == 'forward':  # Text-to-Vision Retrieval\n",
    "        # Sort each row in descending order\n",
    "        indice_matrix = score_matrix.sort(dim=-1, descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "\n",
    "        rank = torch.tensor(rank).to(score_matrix.device)\n",
    "\n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "\n",
    "        eval_log = {\n",
    "            'forward_r1': round(vr_r1 * 100, 1),\n",
    "            'forward_r5': round(vr_r5 * 100, 1),\n",
    "            'forward_r10': round(vr_r10 * 100, 1),\n",
    "            #'forward_recall': f'{round(vr_r1 * 100, 1)}/{round(vr_r5 * 100, 1)}/{round(vr_r10 * 100, 1)}',\n",
    "            'forward_ravg': round((vr_r1 + vr_r5 + vr_r10) / 3 * 100, 1)\n",
    "        }\n",
    "\n",
    "    else:  # Vision-to-Text Retrieval\n",
    "        # Sort each column in descending order\n",
    "        indice_matrix = score_matrix.sort(dim=0, descending=True)[1].permute(1, 0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices = [idx for idx, id_txt in enumerate(ids_txt) if id_txt == ids[i]]\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "\n",
    "        rank = torch.tensor(rank).to(score_matrix.device)\n",
    "\n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "\n",
    "        eval_log = {\n",
    "            'backward_r1': round(tr_r1 * 100, 1),\n",
    "            'backward_r5': round(tr_r5 * 100, 1),\n",
    "            'backward_r10': round(tr_r10 * 100, 1),\n",
    "            'backward_recall': f'{round(tr_r1 * 100,1)}/{round(tr_r5 * 100,1)}/{round(tr_r10 * 100,1)}',\n",
    "            'backward_ravg': round((tr_r1 + tr_r5 + tr_r10) / 3 * 100, 1)\n",
    "        }\n",
    "\n",
    "    return eval_log\n",
    "\n",
    "def compute_gap(feat_modality1: torch.Tensor, feat_modality2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between the centroids of two modalities.\n",
    "\n",
    "    Args:\n",
    "        feat_modality1 (torch.Tensor): Feature matrix of modality 1 with shape [N, D].\n",
    "        feat_modality2 (torch.Tensor): Feature matrix of modality 2 with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Euclidean distance between centroids.\n",
    "    \"\"\"\n",
    "    # Ensure features are normalized if required\n",
    "    modality1_centroid = torch.mean(feat_modality1, dim=0)\n",
    "    modality2_centroid = torch.mean(feat_modality2, dim=0)\n",
    "\n",
    "    gap = modality1_centroid - modality2_centroid\n",
    "    norm_gap = torch.norm(gap).item()\n",
    "\n",
    "    return norm_gap\n",
    "\n",
    "def compute_mean_angular_value_of_a_modality(feat_modality: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean angular value (mean cosine similarity) of a modality.\n",
    "\n",
    "    Args:\n",
    "        feat_modality (torch.Tensor): Feature matrix with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Mean angular value.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = feat_modality @ feat_modality.T\n",
    "\n",
    "    # Exclude diagonal elements by creating a mask\n",
    "    mask = ~torch.eye(cos_sim.size(0), dtype=torch.bool, device=cos_sim.device)\n",
    "    cos_sim_no_diag = cos_sim[mask]\n",
    "\n",
    "    mean_cos_sim = cos_sim_no_diag.mean().item()\n",
    "\n",
    "    return mean_cos_sim\n",
    "\n",
    "def uniformity(features_modality1: torch.Tensor, features_modality2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the uniformity metric for two modalities based on their features.\n",
    "\n",
    "    Args:\n",
    "        features_modality1 (torch.Tensor): Feature matrix of modality 1 with shape [N, D].\n",
    "        features_modality2 (torch.Tensor): Feature matrix of modality 2 with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Uniformity metric (-W2).\n",
    "    \"\"\"\n",
    "    # Concatenate the features of the two modalities\n",
    "    Z = torch.cat([features_modality1, features_modality2], dim=0)  # Shape: [2 * N, D]\n",
    "\n",
    "    # Compute the sample mean \\mu_hat and covariance \\Sigma\n",
    "    mu_hat = torch.mean(Z, dim=0)  # Shape: [D]\n",
    "    Sigma = torch.cov(Z.T)  # Shape: [D, D]\n",
    "\n",
    "    # Calculate the trace and square root of the covariance matrix\n",
    "    trace_Sigma = torch.trace(Sigma)  # Scalar\n",
    "    sqrt_Sigma = torch.linalg.matrix_power(Sigma, 1 // 2)  # Matrix square root of Sigma, shape: [D, D]\n",
    "    trace_sqrt_Sigma = torch.trace(sqrt_Sigma)  # Scalar\n",
    "\n",
    "    # Dimensionality of the features\n",
    "    m = Z.shape[1]\n",
    "\n",
    "    # Compute the quadratic Wasserstein distance W2\n",
    "    W2 = torch.sqrt(\n",
    "        torch.norm(mu_hat)**2 + 1 + trace_Sigma - (2 / torch.sqrt(torch.tensor(m, dtype=Sigma.dtype))) * trace_sqrt_Sigma\n",
    "    )\n",
    "\n",
    "    # Return the uniformity metric (-W2)\n",
    "    return -W2.item()\n",
    "\n",
    "def mean_distance_of_true_pairs(features_modality1: torch.Tensor, features_modality2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean cosine similarity of true pairs between two modalities.\n",
    "\n",
    "    Args:\n",
    "        features_modality1 (torch.Tensor): Normalized feature matrix of modality 1 with shape [N, D].\n",
    "        features_modality2 (torch.Tensor): Normalized feature matrix of modality 2 with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Mean cosine similarity of true pairs.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = torch.matmul(features_modality1, features_modality2.T)\n",
    "\n",
    "    # Extract diagonal elements (true pairs)\n",
    "    cosine_sim_diag = torch.diag(cosine_sim)\n",
    "\n",
    "    # Compute mean cosine similarity of true pairs\n",
    "    cosine_tv_mean = torch.mean(cosine_sim_diag).item()\n",
    "\n",
    "    return cosine_tv_mean\n",
    "\n",
    "def evaluate_model(model: torch.nn.Module, test_loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics, along with additional metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing all evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare storage for embeddings\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "\n",
    "    # IDs for retrieval\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "\n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move embeddings to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds = text_embeds.cpu()\n",
    "\n",
    "            # Store embeddings\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # Assign unique IDs\n",
    "            bs = images.size(0)\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # Shape: [N, D]\n",
    "    all_text_embeds = torch.cat(all_text_embeds, dim=0)    # Shape: [N, D]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval and metric computations\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Compute retrieval metrics\n",
    "    log_forward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')   # Text-to-Vision\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward') # Vision-to-Text\n",
    "\n",
    "    # Compute additional metrics\n",
    "    gap = compute_gap(all_image_embeds, all_text_embeds)\n",
    "    mean_ang_image = compute_mean_angular_value_of_a_modality(all_image_embeds)\n",
    "    mean_ang_text = compute_mean_angular_value_of_a_modality(all_text_embeds)\n",
    "    uniformity_metric = uniformity(all_image_embeds, all_text_embeds)\n",
    "    mean_cos_true_pairs = mean_distance_of_true_pairs(all_image_embeds, all_text_embeds)\n",
    "\n",
    "    # Combine all metrics into final_log\n",
    "    final_log = {\n",
    "        **log_forward,\n",
    "        **log_backward,\n",
    "        'gap': round(gap, 4),\n",
    "        'mean_angular_value_image': round(mean_ang_image, 4), # round to 4 decimal places\n",
    "        'mean_angular_value_text': round(mean_ang_text, 4),\n",
    "        'uniformity': round(uniformity_metric, 4),\n",
    "        'mean_cosine_similarity_true_pairs': round(mean_cos_true_pairs, 4)\n",
    "    }\n",
    "\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log\n",
    "\n",
    "# Example usage (assuming you have a trained model, test_loader, and device defined)\n",
    "# final_metrics = evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_embeddings(text_embeddings, vision_embeddings, \n",
    "                         sample_size=1000, method='pca', \n",
    "                         title=\"Embeddings Visualization\",\n",
    "                         save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes text and vision embeddings in 2D or 3D using PCA, t-SNE, or UMAP.\n",
    "\n",
    "    Args:\n",
    "        text_embeddings (torch.Tensor): \n",
    "            Shape [N, D] containing text embeddings.\n",
    "        vision_embeddings (torch.Tensor):\n",
    "            Shape [N, D] containing vision/image embeddings.\n",
    "        sample_size (int): \n",
    "            If the embeddings contain more than 'sample_size' samples, \n",
    "            randomly pick this many for faster plotting. Set -1 to use all.\n",
    "        method (str): \n",
    "            \"pca\", \"tsne\", or \"umap\".\n",
    "        title (str): \n",
    "            Title for the plot.\n",
    "        save_path (str, optional): \n",
    "            If provided, saves the plot to this path instead of showing it.\n",
    "    \"\"\"\n",
    "    # Detach from graph and bring to CPU if the tensors require grad\n",
    "    text_np = text_embeddings.detach().cpu().numpy()\n",
    "    vision_np = vision_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Optionally downsample for quicker plotting\n",
    "    if sample_size != -1:\n",
    "        n_text = text_np.shape[0]\n",
    "        n_vision = vision_np.shape[0]\n",
    "\n",
    "        n_samples = min(n_text, n_vision)\n",
    "\n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "            text_np = text_np[indices]\n",
    "            vision_np = vision_np[indices]\n",
    "\n",
    "    # Combine for joint dimensionality reduction\n",
    "    all_data = np.concatenate([text_np, vision_np], axis=0)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=3)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"tsne\":\n",
    "        reducer = TSNE(n_components=3, perplexity=30, max_iter=250, random_state=42)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=3, random_state=42, n_jobs=1)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 'pca', 'tsne', and 'umap' are implemented.\")\n",
    "\n",
    "    # Split back into text and vision\n",
    "    text_reduced = reduced[: len(text_np)]\n",
    "    vision_reduced = reduced[len(text_np):]\n",
    "\n",
    "    # Plot 3D visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(text_reduced[:, 0], text_reduced[:, 1], text_reduced[:, 2], \n",
    "               c='red', alpha=0.6, label='Text')\n",
    "    ax.scatter(vision_reduced[:, 0], vision_reduced[:, 1], vision_reduced[:, 2], \n",
    "               c='blue', alpha=0.6, label='Vision')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        wandb.log({title: wandb.Image(plt)})\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, test_loader, device):\n",
    "\n",
    "    # Create model & transforms from scratch (no pretrained weights) #TODO: Use the tokenizer from the chosen model, not the default one\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "        config[\"model\"],\n",
    "        pretrained=None,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Set up training parameters from the config\n",
    "    lr = config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "\n",
    "    # Move the model to multiple GPUs\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    current_batch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, captions_list in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            current_batch += 1\n",
    "            \n",
    "            # Move data to the primary device\n",
    "            images = images.to(device)\n",
    "            captions = captions_list\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer.tokenize(captions)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Encode image and text\n",
    "            image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "            text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "            \n",
    "            # Compute loss based on the experiment type\n",
    "            if config[\"loss_type\"] == \"anchor\":\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "            elif config[\"loss_type\"] == \"anchor+lunif\":\n",
    "                lunif_img = lunif_loss(image_embeds)\n",
    "                lunif_txt = lunif_loss(text_embeds)\n",
    "                lunif = (lunif_img + lunif_txt) / 2\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif\n",
    "            elif config[\"loss_type\"] == \"lunif(50batch)+frozen(text_embed)\":\n",
    "                if current_batch <= 50:\n",
    "                    lunif_img = lunif_loss(image_embeds)\n",
    "                    lunif_txt = lunif_loss(text_embeds)\n",
    "                    lunif = (lunif_img + lunif_txt) / 2\n",
    "                    loss = lunif\n",
    "                else: # train on anchor loss with frozen text embeddings\n",
    "                    text_embeds = text_embeds.detach()\n",
    "                    loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "                    \n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if current_batch % config[\"visualize_every_n_batches\"] == 0:\n",
    "                 visualize_embeddings(text_embeds, \n",
    "                     image_embeds, \n",
    "                     sample_size=1000, \n",
    "                     method='umap', \n",
    "                     title=\"CLIP Embeddings Visualization\",\n",
    "                     save_path=\"embeddings_plot.png\")\n",
    "            \n",
    "            if current_batch % config[\"evaluate_every_n_batches\"] == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "                print(\"Evaluating model...\")\n",
    "                test_results = evaluate_model(model, test_loader, device)\n",
    "                \n",
    "                wandb.log(test_results)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(config):\n",
    "\n",
    "    # Path to train images and annotations\n",
    "    train_image_dir = './data/coco/images/train2017/'                          # Path to train2017 images\n",
    "    train_annotation_file = './data/coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "    # Path to test (val) images and annotations\n",
    "    test_image_dir = './data/coco/images/val2017/'                          # Path to val2017 images\n",
    "    test_annotation_file = './data/coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "    # Define the transform to be applied to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to the model's required input size\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_coco = dset.CocoCaptions(\n",
    "        root=train_image_dir,\n",
    "        annFile=train_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_coco = dset.CocoCaptions(\n",
    "        root=test_image_dir,\n",
    "        annFile=test_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    if config[\"num_train_samples\"] != -1:\n",
    "        print(f\"Subsetting the training dataset to {config['num_train_samples']} samples\")\n",
    "        # Subset the training dataset\n",
    "        num_training_samples = config[\"num_train_samples\"]\n",
    "        subset_indices = list(range(num_training_samples))\n",
    "        train_coco = Subset(train_coco, subset_indices)\n",
    "    \n",
    "    if config[\"num_test_samples\"] != -1:\n",
    "        print(f\"Subsetting the test dataset to {config['num_test_samples']} samples\")\n",
    "        # Subset the test dataset\n",
    "        num_test_samples = config[\"num_test_samples\"]\n",
    "        subset_indices = list(range(num_test_samples))\n",
    "        test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "    # Every image has 5 captions at max, we need to sample one of them\n",
    "    # Create collate function to sample one caption per image\n",
    "    def collate_fn(batch):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        sel_captions = []\n",
    "        for list_captions in captions:\n",
    "            caption = random.choice(list_captions)\n",
    "            sel_captions.append(caption)\n",
    "        return images, sel_captions\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # NumPy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU random numbers\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU random numbers for a single GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random numbers for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark for deterministic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # Finish any existing W&B runs before starting a new one\n",
    "    wandb.finish()\n",
    "\n",
    "    # Initialize your W&B run\n",
    "    wandb.init(project=\"sparsify-clip\", config=config, name=config[\"run_name\"])\n",
    "    \n",
    "    # Print the config\n",
    "    print(\"Config:\", config)\n",
    "    \n",
    "    # Set the device\n",
    "    device_id = config[\"device_id\"]\n",
    "    device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"\\nLoading the dataset...\")\n",
    "    train_loader, test_loader = dataset_loader(config)\n",
    "    print(\"Dataset loaded.\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(config, train_loader, test_loader, device)\n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    print(\"Final evaluation of the model...\")\n",
    "    final_log = evaluate_model(model, test_loader, device)\n",
    "    print(\"Evaluation complete.\\n\")\n",
    "    \n",
    "    # Save the model and upload it to W&B\n",
    "    #torch.save(model.state_dict(), config[\"run_name\"] + \".pt\")\n",
    "    #wandb.save(config[\"run_name\"] + \".pt\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250102_211937-zg7ddkj8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/zg7ddkj8' target=\"_blank\">CLIP-2025-01-02-21-19-36</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/zg7ddkj8' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/zg7ddkj8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'CLIP-2025-01-02-21-19-36', 'device_id': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 1, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor', 'num_train_samples': -1, 'num_test_samples': -1, 'evaluate_every_n_batches': 30, 'visualize_every_n_batches': 30}\n",
      "\n",
      "Loading the dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded.\n",
      "\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/924 [00:16<07:50,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 30  Loss: 4.66858\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      "  3%|▎         | 30/924 [00:32<1:16:31,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.0, 'forward_r5': 0.2, 'forward_r10': 0.4, 'forward_ravg': 0.2, 'backward_r1': 0.0, 'backward_r5': 0.3, 'backward_r10': 0.4, 'backward_recall': '0.0/0.3/0.4', 'backward_ravg': 0.2, 'gap': 1.307, 'mean_angular_value_image': 0.969, 'mean_angular_value_text': 0.9454, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1086}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 59/924 [00:47<07:22,  1.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 60  Loss: 4.67551\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.16it/s]\n",
      "  6%|▋         | 60/924 [01:02<1:12:06,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.0, 'forward_r5': 0.2, 'forward_r10': 0.6, 'forward_ravg': 0.3, 'backward_r1': 0.1, 'backward_r5': 0.3, 'backward_r10': 0.7, 'backward_recall': '0.1/0.3/0.7', 'backward_ravg': 0.4, 'gap': 1.2163, 'mean_angular_value_image': 0.9119, 'mean_angular_value_text': 0.9344, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2074}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 89/924 [01:17<07:13,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 90  Loss: 4.52909\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 10%|▉         | 90/924 [01:32<1:09:37,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.3, 'forward_r10': 0.8, 'forward_ravg': 0.4, 'backward_r1': 0.1, 'backward_r5': 0.5, 'backward_r10': 1.0, 'backward_recall': '0.1/0.5/1.0', 'backward_ravg': 0.5, 'gap': 1.1108, 'mean_angular_value_image': 0.8419, 'mean_angular_value_text': 0.8537, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 119/924 [01:47<06:55,  1.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 120  Loss: 4.49053\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 13%|█▎        | 120/924 [02:03<1:07:53,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.5, 'forward_r10': 0.8, 'forward_ravg': 0.5, 'backward_r1': 0.1, 'backward_r5': 0.8, 'backward_r10': 1.3, 'backward_recall': '0.1/0.8/1.3', 'backward_ravg': 0.7, 'gap': 1.0501, 'mean_angular_value_image': 0.8281, 'mean_angular_value_text': 0.865, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.3561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 149/924 [02:18<06:44,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 150  Loss: 4.47749\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.09it/s]\n",
      " 16%|█▌        | 150/924 [02:34<1:06:49,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.6, 'forward_r10': 1.1, 'forward_ravg': 0.6, 'backward_r1': 0.1, 'backward_r5': 0.8, 'backward_r10': 1.6, 'backward_recall': '0.1/0.8/1.6', 'backward_ravg': 0.8, 'gap': 1.0092, 'mean_angular_value_image': 0.7581, 'mean_angular_value_text': 0.8325, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.3682}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 179/924 [02:49<06:31,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 180  Loss: 4.38186\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.11it/s]\n",
      " 19%|█▉        | 180/924 [03:04<1:02:56,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.5, 'forward_r10': 1.1, 'forward_ravg': 0.6, 'backward_r1': 0.3, 'backward_r5': 0.8, 'backward_r10': 1.2, 'backward_recall': '0.3/0.8/1.2', 'backward_ravg': 0.8, 'gap': 0.9981, 'mean_angular_value_image': 0.7532, 'mean_angular_value_text': 0.8416, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 209/924 [03:19<06:11,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 210  Loss: 4.31480\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.13it/s]\n",
      " 23%|██▎       | 210/924 [03:35<1:00:14,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.7, 'forward_r10': 1.6, 'forward_ravg': 0.8, 'backward_r1': 0.2, 'backward_r5': 0.7, 'backward_r10': 1.3, 'backward_recall': '0.2/0.7/1.3', 'backward_ravg': 0.8, 'gap': 0.9769, 'mean_angular_value_image': 0.7245, 'mean_angular_value_text': 0.803, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.3875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 239/924 [03:49<05:56,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 240  Loss: 4.09162\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      " 26%|██▌       | 240/924 [04:05<57:55,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.7, 'forward_r10': 1.7, 'forward_ravg': 0.8, 'backward_r1': 0.2, 'backward_r5': 0.6, 'backward_r10': 1.4, 'backward_recall': '0.2/0.6/1.4', 'backward_ravg': 0.7, 'gap': 0.9029, 'mean_angular_value_image': 0.6878, 'mean_angular_value_text': 0.7842, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 269/924 [04:20<05:38,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 270  Loss: 4.12768\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 29%|██▉       | 270/924 [04:36<55:18,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 1.0, 'forward_r10': 1.9, 'forward_ravg': 1.0, 'backward_r1': 0.3, 'backward_r5': 1.0, 'backward_r10': 1.7, 'backward_recall': '0.3/1.0/1.7', 'backward_ravg': 1.0, 'gap': 0.9052, 'mean_angular_value_image': 0.6554, 'mean_angular_value_text': 0.788, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4235}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 299/924 [04:50<05:25,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 300  Loss: 3.98940\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.11it/s]\n",
      " 32%|███▏      | 300/924 [05:06<52:34,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.2, 'forward_r5': 1.1, 'forward_r10': 2.3, 'forward_ravg': 1.2, 'backward_r1': 0.2, 'backward_r5': 1.2, 'backward_r10': 2.6, 'backward_recall': '0.2/1.2/2.6', 'backward_ravg': 1.3, 'gap': 0.9431, 'mean_angular_value_image': 0.6749, 'mean_angular_value_text': 0.7609, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.3931}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 329/924 [05:21<05:07,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 330  Loss: 4.12866\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 36%|███▌      | 330/924 [05:36<49:45,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.4, 'forward_r5': 1.2, 'forward_r10': 2.2, 'forward_ravg': 1.2, 'backward_r1': 0.4, 'backward_r5': 1.3, 'backward_r10': 2.3, 'backward_recall': '0.4/1.3/2.3', 'backward_ravg': 1.3, 'gap': 0.8817, 'mean_angular_value_image': 0.6113, 'mean_angular_value_text': 0.742, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4212}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 359/924 [05:51<04:54,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 360  Loss: 3.91214\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.07it/s]\n",
      " 39%|███▉      | 360/924 [06:07<48:13,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.2, 'forward_r5': 1.0, 'forward_r10': 2.0, 'forward_ravg': 1.1, 'backward_r1': 0.4, 'backward_r5': 1.5, 'backward_r10': 2.6, 'backward_recall': '0.4/1.5/2.6', 'backward_ravg': 1.5, 'gap': 0.8237, 'mean_angular_value_image': 0.5679, 'mean_angular_value_text': 0.6577, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 389/924 [06:22<04:36,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 390  Loss: 4.16565\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.13it/s]\n",
      " 42%|████▏     | 390/924 [06:37<44:51,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_r5': 1.5, 'forward_r10': 2.8, 'forward_ravg': 1.6, 'backward_r1': 0.4, 'backward_r5': 1.8, 'backward_r10': 3.1, 'backward_recall': '0.4/1.8/3.1', 'backward_ravg': 1.8, 'gap': 0.8121, 'mean_angular_value_image': 0.4887, 'mean_angular_value_text': 0.6612, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 419/924 [06:52<04:21,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 420  Loss: 3.69120\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 45%|████▌     | 420/924 [07:08<42:15,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_r5': 1.9, 'forward_r10': 3.3, 'forward_ravg': 1.8, 'backward_r1': 0.3, 'backward_r5': 1.3, 'backward_r10': 2.8, 'backward_recall': '0.3/1.3/2.8', 'backward_ravg': 1.5, 'gap': 0.7847, 'mean_angular_value_image': 0.5307, 'mean_angular_value_text': 0.678, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 449/924 [07:22<04:06,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 450  Loss: 3.80907\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 49%|████▊     | 450/924 [07:38<40:03,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.4, 'forward_r5': 1.8, 'forward_r10': 3.3, 'forward_ravg': 1.8, 'backward_r1': 0.5, 'backward_r5': 1.9, 'backward_r10': 3.3, 'backward_recall': '0.5/1.9/3.3', 'backward_ravg': 1.9, 'gap': 0.7714, 'mean_angular_value_image': 0.4515, 'mean_angular_value_text': 0.6655, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4399}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 479/924 [07:53<03:49,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 480  Loss: 3.85243\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 52%|█████▏    | 480/924 [08:09<37:48,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.1, 'forward_r10': 3.6, 'forward_ravg': 2.1, 'backward_r1': 0.5, 'backward_r5': 1.9, 'backward_r10': 3.5, 'backward_recall': '0.5/1.9/3.5', 'backward_ravg': 2.0, 'gap': 0.7914, 'mean_angular_value_image': 0.5074, 'mean_angular_value_text': 0.6824, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 509/924 [08:23<03:34,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 510  Loss: 3.65467\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 55%|█████▌    | 510/924 [08:39<34:59,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.4, 'forward_r10': 4.2, 'forward_ravg': 2.4, 'backward_r1': 0.7, 'backward_r5': 2.3, 'backward_r10': 4.1, 'backward_recall': '0.7/2.3/4.1', 'backward_ravg': 2.3, 'gap': 0.7504, 'mean_angular_value_image': 0.4906, 'mean_angular_value_text': 0.6248, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4585}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 539/924 [08:54<03:19,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 540  Loss: 3.87560\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.17it/s]\n",
      " 58%|█████▊    | 540/924 [09:09<32:04,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.4, 'forward_r5': 2.0, 'forward_r10': 3.9, 'forward_ravg': 2.1, 'backward_r1': 0.5, 'backward_r5': 2.2, 'backward_r10': 3.7, 'backward_recall': '0.5/2.2/3.7', 'backward_ravg': 2.1, 'gap': 0.7944, 'mean_angular_value_image': 0.4879, 'mean_angular_value_text': 0.6537, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 569/924 [09:24<03:04,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 570  Loss: 3.64057\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.11it/s]\n",
      " 62%|██████▏   | 570/924 [09:40<29:57,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.4, 'forward_r10': 4.3, 'forward_ravg': 2.4, 'backward_r1': 0.5, 'backward_r5': 2.2, 'backward_r10': 4.2, 'backward_recall': '0.5/2.2/4.2', 'backward_ravg': 2.3, 'gap': 0.7345, 'mean_angular_value_image': 0.4724, 'mean_angular_value_text': 0.6745, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4727}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 599/924 [09:54<02:48,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 600  Loss: 3.61471\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      " 65%|██████▍   | 600/924 [10:10<27:35,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.5, 'forward_r5': 2.3, 'forward_r10': 4.4, 'forward_ravg': 2.4, 'backward_r1': 0.7, 'backward_r5': 2.7, 'backward_r10': 4.8, 'backward_recall': '0.7/2.7/4.8', 'backward_ravg': 2.7, 'gap': 0.6977, 'mean_angular_value_image': 0.3785, 'mean_angular_value_text': 0.6096, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 629/924 [10:25<02:32,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 630  Loss: 3.70060\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.10it/s]\n",
      " 68%|██████▊   | 630/924 [10:41<24:39,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.7, 'forward_r10': 4.6, 'forward_ravg': 2.7, 'backward_r1': 0.6, 'backward_r5': 2.5, 'backward_r10': 4.7, 'backward_recall': '0.6/2.5/4.7', 'backward_ravg': 2.6, 'gap': 0.6921, 'mean_angular_value_image': 0.4382, 'mean_angular_value_text': 0.5744, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4717}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 659/924 [10:55<02:17,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 660  Loss: 3.57458\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 71%|███████▏  | 660/924 [11:11<22:16,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.4, 'forward_r5': 2.4, 'forward_r10': 4.6, 'forward_ravg': 2.5, 'backward_r1': 0.6, 'backward_r5': 2.9, 'backward_r10': 5.0, 'backward_recall': '0.6/2.9/5.0', 'backward_ravg': 2.8, 'gap': 0.6786, 'mean_angular_value_image': 0.3991, 'mean_angular_value_text': 0.5743, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4657}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 689/924 [11:26<02:01,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 690  Loss: 3.45504\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 75%|███████▍  | 690/924 [11:41<19:34,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 3.1, 'forward_r10': 5.4, 'forward_ravg': 3.0, 'backward_r1': 0.5, 'backward_r5': 2.5, 'backward_r10': 4.6, 'backward_recall': '0.5/2.5/4.6', 'backward_ravg': 2.6, 'gap': 0.6667, 'mean_angular_value_image': 0.3623, 'mean_angular_value_text': 0.5204, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4475}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 719/924 [11:56<01:46,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 720  Loss: 3.41312\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 78%|███████▊  | 720/924 [12:11<17:02,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.9, 'forward_r10': 5.4, 'forward_ravg': 3.0, 'backward_r1': 0.8, 'backward_r5': 2.7, 'backward_r10': 5.0, 'backward_recall': '0.8/2.7/5.0', 'backward_ravg': 2.8, 'gap': 0.6403, 'mean_angular_value_image': 0.3483, 'mean_angular_value_text': 0.5953, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4844}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 749/924 [12:26<01:33,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 750  Loss: 3.40776\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  3.91it/s]\n",
      " 81%|████████  | 750/924 [12:42<15:02,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.8, 'forward_r5': 3.2, 'forward_r10': 5.6, 'forward_ravg': 3.2, 'backward_r1': 0.8, 'backward_r5': 3.1, 'backward_r10': 5.4, 'backward_recall': '0.8/3.1/5.4', 'backward_ravg': 3.1, 'gap': 0.5914, 'mean_angular_value_image': 0.2981, 'mean_angular_value_text': 0.5173, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 779/924 [12:57<01:15,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 780  Loss: 3.40646\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 84%|████████▍ | 780/924 [13:12<12:02,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.7, 'forward_r5': 3.3, 'forward_r10': 5.7, 'forward_ravg': 3.2, 'backward_r1': 0.8, 'backward_r5': 3.3, 'backward_r10': 6.0, 'backward_recall': '0.8/3.3/6.0', 'backward_ravg': 3.4, 'gap': 0.6253, 'mean_angular_value_image': 0.3413, 'mean_angular_value_text': 0.5337, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4753}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 809/924 [13:27<00:59,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 810  Loss: 3.51110\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 88%|████████▊ | 810/924 [13:43<09:30,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.9, 'forward_r5': 3.6, 'forward_r10': 6.0, 'forward_ravg': 3.5, 'backward_r1': 0.8, 'backward_r5': 3.4, 'backward_r10': 6.5, 'backward_recall': '0.8/3.4/6.5', 'backward_ravg': 3.6, 'gap': 0.6217, 'mean_angular_value_image': 0.3144, 'mean_angular_value_text': 0.5376, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4721}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 839/924 [13:57<00:44,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 840  Loss: 3.54169\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 91%|█████████ | 840/924 [14:13<06:59,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.8, 'forward_r5': 3.3, 'forward_r10': 6.2, 'forward_ravg': 3.5, 'backward_r1': 0.7, 'backward_r5': 3.0, 'backward_r10': 5.6, 'backward_recall': '0.7/3.0/5.6', 'backward_ravg': 3.1, 'gap': 0.64, 'mean_angular_value_image': 0.3852, 'mean_angular_value_text': 0.5128, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4719}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 869/924 [14:28<00:28,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 870  Loss: 3.29147\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.16it/s]\n",
      " 94%|█████████▍| 870/924 [14:43<04:30,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 3.4, 'forward_r10': 5.9, 'forward_ravg': 3.4, 'backward_r1': 0.9, 'backward_r5': 3.7, 'backward_r10': 6.1, 'backward_recall': '0.9/3.7/6.1', 'backward_ravg': 3.6, 'gap': 0.5709, 'mean_angular_value_image': 0.3094, 'mean_angular_value_text': 0.4981, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4891}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 899/924 [14:58<00:12,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 900  Loss: 3.45712\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 97%|█████████▋| 900/924 [15:13<01:59,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.1, 'forward_r5': 4.0, 'forward_r10': 6.9, 'forward_ravg': 4.0, 'backward_r1': 1.1, 'backward_r5': 3.9, 'backward_r10': 7.1, 'backward_recall': '1.1/3.9/7.1', 'backward_ravg': 4.1, 'gap': 0.5993, 'mean_angular_value_image': 0.3355, 'mean_angular_value_text': 0.5235, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [15:25<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Loss: 3.5070\n",
      "Training complete.\n",
      "\n",
      "Final evaluation of the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.9, 'forward_r5': 3.4, 'forward_r10': 5.9, 'forward_ravg': 3.4, 'backward_r1': 0.8, 'backward_r5': 3.7, 'backward_r10': 6.1, 'backward_recall': '0.8/3.7/6.1', 'backward_ravg': 3.5, 'gap': 0.5453, 'mean_angular_value_image': 0.2943, 'mean_angular_value_text': 0.4847, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.4946}\n",
      "Evaluation complete.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>▁▂▂▂▂▃▂▂▃▂▄▄▄▃▄▄▅▄▄▅▅▅▄▆▆▆▆▅▇█</td></tr><tr><td>backward_r10</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▄▅▆▅▆▅▆▆▇▇▆▇█</td></tr><tr><td>backward_r5</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▃▄▄▅▅▅▆▅▆▅▆▆▇▇▆██</td></tr><tr><td>backward_ravg</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▄▃▄▄▅▄▅▅▅▆▅▆▆▇▇▆▇█</td></tr><tr><td>forward_r1</td><td>▁▁▂▂▂▂▂▂▂▂▄▂▃▃▄▅▅▄▅▄▅▄▅▅▆▅▇▆▇█</td></tr><tr><td>forward_r10</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>forward_r5</td><td>▁▁▁▂▂▂▂▂▂▃▃▂▃▄▄▄▅▄▅▅▆▅▆▆▇▇▇▇▇█</td></tr><tr><td>forward_ravg</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▅▄▅▅▆▅▆▆▇▇▇▇▇█</td></tr><tr><td>gap</td><td>█▇▆▆▅▅▅▄▄▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▁▂▁▁</td></tr><tr><td>mean_angular_value_image</td><td>█▇▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>mean_angular_value_text</td><td>██▇▇▆▆▆▅▆▅▅▃▄▄▄▄▃▃▄▃▂▂▁▃▁▂▂▁▁▁</td></tr><tr><td>mean_cosine_similarity_true_pairs</td><td>▁▃▄▅▆▆▆▇▇▆▇▇▆▇▇▇▇▇█▇█▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▅▆▆▆▆▅▆▅▅▄▅▅▄▅▅▄▄▄▃▃▄▃▃▃▃▃▂▂▂▃▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>1.1</td></tr><tr><td>backward_r10</td><td>7.1</td></tr><tr><td>backward_r5</td><td>3.9</td></tr><tr><td>backward_ravg</td><td>4.1</td></tr><tr><td>backward_recall</td><td>1.1/3.9/7.1</td></tr><tr><td>forward_r1</td><td>1.1</td></tr><tr><td>forward_r10</td><td>6.9</td></tr><tr><td>forward_r5</td><td>4</td></tr><tr><td>forward_ravg</td><td>4</td></tr><tr><td>gap</td><td>0.5993</td></tr><tr><td>mean_angular_value_image</td><td>0.3355</td></tr><tr><td>mean_angular_value_text</td><td>0.5235</td></tr><tr><td>mean_cosine_similarity_true_pairs</td><td>0.4946</td></tr><tr><td>train_loss</td><td>3.50698</td></tr><tr><td>uniformity</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP-2025-01-02-21-19-36</strong> at: <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/zg7ddkj8' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/zg7ddkj8</a><br> View project at: <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 30 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250102_211937-zg7ddkj8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Anchor + Lunif model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250102_213522-ntlwxocu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/ntlwxocu' target=\"_blank\">CLIP-2025-01-02-21-19-36</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/ntlwxocu' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/ntlwxocu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'CLIP-2025-01-02-21-19-36', 'device_id': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 1, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor+lunif', 'num_train_samples': -1, 'num_test_samples': -1, 'evaluate_every_n_batches': 30, 'visualize_every_n_batches': 30}\n",
      "\n",
      "Loading the dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=0.71s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded.\n",
      "\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/924 [00:16<08:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 30  Loss: 1.26669\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.06it/s]\n",
      "  3%|▎         | 30/924 [00:33<1:17:31,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.0, 'forward_r5': 0.2, 'forward_r10': 0.4, 'forward_ravg': 0.2, 'backward_r1': 0.1, 'backward_r5': 0.2, 'backward_r10': 0.3, 'backward_recall': '0.1/0.2/0.3', 'backward_ravg': 0.2, 'gap': 0.8923, 'mean_angular_value_image': 0.7963, 'mean_angular_value_text': 0.037, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.0257}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 59/924 [00:47<07:30,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 60  Loss: 1.44125\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      "  6%|▋         | 60/924 [01:03<1:13:02,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.6, 'forward_r10': 1.1, 'forward_ravg': 0.6, 'backward_r1': 0.1, 'backward_r5': 0.5, 'backward_r10': 0.9, 'backward_recall': '0.1/0.5/0.9', 'backward_ravg': 0.5, 'gap': 0.3031, 'mean_angular_value_image': 0.0742, 'mean_angular_value_text': 0.0262, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.0496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 89/924 [01:18<07:13,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 90  Loss: 1.07148\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.07it/s]\n",
      " 10%|▉         | 90/924 [01:33<1:10:21,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 0.7, 'forward_r10': 1.4, 'forward_ravg': 0.7, 'backward_r1': 0.1, 'backward_r5': 0.6, 'backward_r10': 1.6, 'backward_recall': '0.1/0.6/1.6', 'backward_ravg': 0.8, 'gap': 0.2509, 'mean_angular_value_image': 0.0394, 'mean_angular_value_text': 0.025, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.0651}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 119/924 [01:48<06:56,  1.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 120  Loss: 0.88426\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 13%|█▎        | 120/924 [02:04<1:07:43,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.2, 'forward_r5': 1.0, 'forward_r10': 1.6, 'forward_ravg': 1.0, 'backward_r1': 0.2, 'backward_r5': 0.8, 'backward_r10': 1.4, 'backward_recall': '0.2/0.8/1.4', 'backward_ravg': 0.8, 'gap': 0.231, 'mean_angular_value_image': 0.033, 'mean_angular_value_text': 0.0234, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 149/924 [02:19<06:44,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 150  Loss: 0.71253\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.03it/s]\n",
      " 16%|█▌        | 150/924 [02:35<1:07:28,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.2, 'forward_r5': 1.0, 'forward_r10': 1.7, 'forward_ravg': 1.0, 'backward_r1': 0.2, 'backward_r5': 0.8, 'backward_r10': 1.7, 'backward_recall': '0.2/0.8/1.7', 'backward_ravg': 0.9, 'gap': 0.2299, 'mean_angular_value_image': 0.0279, 'mean_angular_value_text': 0.0214, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.0856}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 179/924 [02:50<06:27,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 180  Loss: 0.91073\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 19%|█▉        | 180/924 [03:06<1:04:32,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_r5': 1.1, 'forward_r10': 2.5, 'forward_ravg': 1.3, 'backward_r1': 0.4, 'backward_r5': 1.0, 'backward_r10': 1.9, 'backward_recall': '0.4/1.0/1.9', 'backward_ravg': 1.1, 'gap': 0.2135, 'mean_angular_value_image': 0.0334, 'mean_angular_value_text': 0.0145, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.0953}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 209/924 [03:21<06:11,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 210  Loss: 0.58839\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.10it/s]\n",
      " 23%|██▎       | 210/924 [03:37<1:01:05,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_r5': 1.5, 'forward_r10': 2.8, 'forward_ravg': 1.5, 'backward_r1': 0.3, 'backward_r5': 1.1, 'backward_r10': 2.1, 'backward_recall': '0.3/1.1/2.1', 'backward_ravg': 1.2, 'gap': 0.2063, 'mean_angular_value_image': 0.0251, 'mean_angular_value_text': 0.0158, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1067}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 239/924 [03:51<05:56,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 240  Loss: 0.41345\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.11it/s]\n",
      " 26%|██▌       | 240/924 [04:07<58:47,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.5, 'forward_r5': 1.7, 'forward_r10': 2.9, 'forward_ravg': 1.7, 'backward_r1': 0.3, 'backward_r5': 1.5, 'backward_r10': 2.9, 'backward_recall': '0.3/1.5/2.9', 'backward_ravg': 1.6, 'gap': 0.1978, 'mean_angular_value_image': 0.026, 'mean_angular_value_text': 0.0148, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1207}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 269/924 [04:22<05:38,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 270  Loss: 0.24120\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 29%|██▉       | 270/924 [04:38<54:39,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_r5': 1.6, 'forward_r10': 3.4, 'forward_ravg': 1.7, 'backward_r1': 0.4, 'backward_r5': 1.6, 'backward_r10': 3.6, 'backward_recall': '0.4/1.6/3.6', 'backward_ravg': 1.9, 'gap': 0.1889, 'mean_angular_value_image': 0.0204, 'mean_angular_value_text': 0.0134, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1217}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 299/924 [04:52<05:23,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 300  Loss: 0.20345\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.13it/s]\n",
      " 32%|███▏      | 300/924 [05:09<54:24,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.3, 'forward_r10': 3.8, 'forward_ravg': 2.2, 'backward_r1': 0.5, 'backward_r5': 2.4, 'backward_r10': 4.3, 'backward_recall': '0.5/2.4/4.3', 'backward_ravg': 2.4, 'gap': 0.184, 'mean_angular_value_image': 0.0229, 'mean_angular_value_text': 0.01, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 329/924 [05:23<05:09,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 330  Loss: 0.27945\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 36%|███▌      | 330/924 [05:39<49:56,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.4, 'forward_r5': 2.2, 'forward_r10': 4.1, 'forward_ravg': 2.3, 'backward_r1': 0.4, 'backward_r5': 2.0, 'backward_r10': 3.9, 'backward_recall': '0.4/2.0/3.9', 'backward_ravg': 2.1, 'gap': 0.2069, 'mean_angular_value_image': 0.0266, 'mean_angular_value_text': 0.0149, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1381}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 359/924 [05:54<04:53,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 360  Loss: 0.00364\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.09it/s]\n",
      " 39%|███▉      | 360/924 [06:09<47:45,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.2, 'forward_r10': 4.2, 'forward_ravg': 2.3, 'backward_r1': 0.4, 'backward_r5': 2.2, 'backward_r10': 4.0, 'backward_recall': '0.4/2.2/4.0', 'backward_ravg': 2.2, 'gap': 0.193, 'mean_angular_value_image': 0.0264, 'mean_angular_value_text': 0.0093, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1478}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 389/924 [06:24<04:37,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 390  Loss: 0.45124\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      " 42%|████▏     | 390/924 [06:40<45:34,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.7, 'forward_r5': 2.2, 'forward_r10': 4.2, 'forward_ravg': 2.4, 'backward_r1': 0.7, 'backward_r5': 2.1, 'backward_r10': 4.2, 'backward_recall': '0.7/2.1/4.2', 'backward_ravg': 2.4, 'gap': 0.1929, 'mean_angular_value_image': 0.0227, 'mean_angular_value_text': 0.0164, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1582}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 419/924 [06:55<04:22,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 420  Loss: 0.09275\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 45%|████▌     | 420/924 [07:10<42:10,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.6, 'forward_r5': 2.7, 'forward_r10': 4.6, 'forward_ravg': 2.6, 'backward_r1': 0.7, 'backward_r5': 2.3, 'backward_r10': 4.5, 'backward_recall': '0.7/2.3/4.5', 'backward_ravg': 2.5, 'gap': 0.1853, 'mean_angular_value_image': 0.021, 'mean_angular_value_text': 0.0125, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1599}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 449/924 [07:25<04:07,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 450  Loss: -0.19277\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 49%|████▊     | 450/924 [07:41<39:41,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.8, 'forward_r5': 2.8, 'forward_r10': 5.1, 'forward_ravg': 2.9, 'backward_r1': 0.8, 'backward_r5': 3.2, 'backward_r10': 5.5, 'backward_recall': '0.8/3.2/5.5', 'backward_ravg': 3.1, 'gap': 0.1726, 'mean_angular_value_image': 0.0216, 'mean_angular_value_text': 0.0087, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1599}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 479/924 [07:55<03:50,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 480  Loss: 0.11901\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.07it/s]\n",
      " 52%|█████▏    | 480/924 [08:11<37:29,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.9, 'forward_r5': 3.1, 'forward_r10': 5.5, 'forward_ravg': 3.2, 'backward_r1': 0.8, 'backward_r5': 2.8, 'backward_r10': 5.1, 'backward_recall': '0.8/2.8/5.1', 'backward_ravg': 2.9, 'gap': 0.1484, 'mean_angular_value_image': 0.0184, 'mean_angular_value_text': 0.0075, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1729}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 509/924 [08:26<03:35,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 510  Loss: -0.24350\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 55%|█████▌    | 510/924 [08:41<34:40,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.8, 'forward_r5': 3.5, 'forward_r10': 6.1, 'forward_ravg': 3.5, 'backward_r1': 0.6, 'backward_r5': 2.9, 'backward_r10': 5.4, 'backward_recall': '0.6/2.9/5.4', 'backward_ravg': 3.0, 'gap': 0.1777, 'mean_angular_value_image': 0.0209, 'mean_angular_value_text': 0.0098, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 539/924 [08:56<03:18,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 540  Loss: 0.10842\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.05it/s]\n",
      " 58%|█████▊    | 540/924 [09:12<32:36,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 3.3, 'forward_r10': 5.5, 'forward_ravg': 3.3, 'backward_r1': 0.7, 'backward_r5': 3.2, 'backward_r10': 5.8, 'backward_recall': '0.7/3.2/5.8', 'backward_ravg': 3.2, 'gap': 0.1813, 'mean_angular_value_image': 0.0233, 'mean_angular_value_text': 0.0108, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 569/924 [09:27<03:03,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 570  Loss: -0.14703\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 62%|██████▏   | 570/924 [09:42<30:07,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 3.2, 'forward_r10': 5.9, 'forward_ravg': 3.4, 'backward_r1': 0.7, 'backward_r5': 3.2, 'backward_r10': 5.8, 'backward_recall': '0.7/3.2/5.8', 'backward_ravg': 3.3, 'gap': 0.1806, 'mean_angular_value_image': 0.0269, 'mean_angular_value_text': 0.0092, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1782}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 599/924 [09:57<02:48,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 600  Loss: -0.09571\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.14it/s]\n",
      " 65%|██████▍   | 600/924 [10:13<27:03,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 3.6, 'forward_r10': 6.4, 'forward_ravg': 3.7, 'backward_r1': 0.7, 'backward_r5': 3.2, 'backward_r10': 6.1, 'backward_recall': '0.7/3.2/6.1', 'backward_ravg': 3.3, 'gap': 0.1682, 'mean_angular_value_image': 0.0214, 'mean_angular_value_text': 0.01, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1842}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 629/924 [10:27<02:33,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 630  Loss: -0.19540\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.07it/s]\n",
      " 68%|██████▊   | 630/924 [10:43<25:06,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 3.7, 'forward_r10': 6.5, 'forward_ravg': 3.7, 'backward_r1': 0.8, 'backward_r5': 3.4, 'backward_r10': 6.4, 'backward_recall': '0.8/3.4/6.4', 'backward_ravg': 3.5, 'gap': 0.1772, 'mean_angular_value_image': 0.0248, 'mean_angular_value_text': 0.0114, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 659/924 [10:58<02:17,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 660  Loss: -0.09929\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.06it/s]\n",
      " 71%|███████▏  | 660/924 [11:14<22:44,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.0, 'forward_r5': 4.0, 'forward_r10': 6.7, 'forward_ravg': 3.9, 'backward_r1': 0.8, 'backward_r5': 3.3, 'backward_r10': 6.4, 'backward_recall': '0.8/3.3/6.4', 'backward_ravg': 3.5, 'gap': 0.1561, 'mean_angular_value_image': 0.0222, 'mean_angular_value_text': 0.0092, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.199}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 689/924 [11:29<02:02,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 690  Loss: -0.30728\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.04it/s]\n",
      " 75%|███████▍  | 690/924 [11:45<20:09,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.1, 'forward_r5': 4.2, 'forward_r10': 7.5, 'forward_ravg': 4.3, 'backward_r1': 1.0, 'backward_r5': 4.0, 'backward_r10': 7.1, 'backward_recall': '1.0/4.0/7.1', 'backward_ravg': 4.0, 'gap': 0.1635, 'mean_angular_value_image': 0.0209, 'mean_angular_value_text': 0.0058, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2029}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 719/924 [12:00<01:46,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 720  Loss: -0.28695\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 78%|███████▊  | 720/924 [12:15<17:17,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.1, 'forward_r5': 4.2, 'forward_r10': 7.0, 'forward_ravg': 4.1, 'backward_r1': 1.1, 'backward_r5': 3.7, 'backward_r10': 6.8, 'backward_recall': '1.1/3.7/6.8', 'backward_ravg': 3.9, 'gap': 0.1588, 'mean_angular_value_image': 0.0169, 'mean_angular_value_text': 0.0099, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.1984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 749/924 [12:30<01:30,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 750  Loss: -0.31633\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      " 81%|████████  | 750/924 [12:46<14:51,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.2, 'forward_r5': 4.5, 'forward_r10': 7.7, 'forward_ravg': 4.5, 'backward_r1': 1.0, 'backward_r5': 4.5, 'backward_r10': 7.5, 'backward_recall': '1.0/4.5/7.5', 'backward_ravg': 4.4, 'gap': 0.1804, 'mean_angular_value_image': 0.0234, 'mean_angular_value_text': 0.0083, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 779/924 [13:01<01:14,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 780  Loss: -0.43263\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.15it/s]\n",
      " 84%|████████▍ | 780/924 [13:16<12:05,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.3, 'forward_r5': 4.5, 'forward_r10': 7.8, 'forward_ravg': 4.5, 'backward_r1': 1.4, 'backward_r5': 4.6, 'backward_r10': 7.4, 'backward_recall': '1.4/4.6/7.4', 'backward_ravg': 4.5, 'gap': 0.1562, 'mean_angular_value_image': 0.0191, 'mean_angular_value_text': 0.0089, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.207}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 809/924 [13:31<00:59,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 810  Loss: -0.26793\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.13it/s]\n",
      " 88%|████████▊ | 810/924 [13:47<09:33,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.3, 'forward_r5': 4.7, 'forward_r10': 8.0, 'forward_ravg': 4.7, 'backward_r1': 1.3, 'backward_r5': 5.0, 'backward_r10': 8.2, 'backward_recall': '1.3/5.0/8.2', 'backward_ravg': 4.8, 'gap': 0.1576, 'mean_angular_value_image': 0.0176, 'mean_angular_value_text': 0.0079, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 839/924 [14:01<00:43,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 840  Loss: -0.24267\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.13it/s]\n",
      " 91%|█████████ | 840/924 [14:17<07:04,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.2, 'forward_r5': 4.6, 'forward_r10': 8.3, 'forward_ravg': 4.7, 'backward_r1': 1.2, 'backward_r5': 4.7, 'backward_r10': 7.9, 'backward_recall': '1.2/4.7/7.9', 'backward_ravg': 4.6, 'gap': 0.1651, 'mean_angular_value_image': 0.0181, 'mean_angular_value_text': 0.0084, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2072}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 869/924 [14:32<00:28,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 870  Loss: -0.46873\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 94%|█████████▍| 870/924 [14:47<04:32,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.2, 'forward_r5': 4.8, 'forward_r10': 8.2, 'forward_ravg': 4.7, 'backward_r1': 1.1, 'backward_r5': 4.6, 'backward_r10': 8.0, 'backward_recall': '1.1/4.6/8.0', 'backward_ravg': 4.6, 'gap': 0.1702, 'mean_angular_value_image': 0.0274, 'mean_angular_value_text': 0.0072, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 899/924 [15:02<00:12,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 900  Loss: -0.32680\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.12it/s]\n",
      " 97%|█████████▋| 900/924 [15:18<02:03,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.1, 'forward_r5': 5.1, 'forward_r10': 8.7, 'forward_ravg': 5.0, 'backward_r1': 1.3, 'backward_r5': 4.7, 'backward_r10': 8.0, 'backward_recall': '1.3/4.7/8.0', 'backward_ravg': 4.7, 'gap': 0.1992, 'mean_angular_value_image': 0.0251, 'mean_angular_value_text': 0.0113, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2085}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [15:30<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Loss: -0.1194\n",
      "Training complete.\n",
      "\n",
      "Final evaluation of the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 1.4, 'forward_r5': 4.8, 'forward_r10': 8.4, 'forward_ravg': 4.9, 'backward_r1': 1.1, 'backward_r5': 4.9, 'backward_r10': 8.2, 'backward_recall': '1.1/4.9/8.2', 'backward_ravg': 4.7, 'gap': 0.1592, 'mean_angular_value_image': 0.0187, 'mean_angular_value_text': 0.0075, 'uniformity': nan, 'mean_cosine_similarity_true_pairs': 0.2217}\n",
      "Evaluation complete.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>▁▁▁▂▂▃▂▂▃▃▃▃▄▄▅▅▄▄▄▄▅▅▆▆▆█▇▇▆▇</td></tr><tr><td>backward_r10</td><td>▁▂▂▂▂▂▃▃▄▅▄▄▄▅▆▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>backward_r5</td><td>▁▁▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▇▆▇▇██▇█</td></tr><tr><td>backward_ravg</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇█████</td></tr><tr><td>forward_r1</td><td>▁▂▂▂▂▂▃▄▃▄▃▄▅▄▅▆▅▆▆▆▆▆▇▇▇██▇▇▇</td></tr><tr><td>forward_r10</td><td>▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>forward_r5</td><td>▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▅▅▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>forward_ravg</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>gap</td><td>█▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_angular_value_image</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_angular_value_text</td><td>█▆▅▅▅▃▃▃▃▂▃▂▃▃▂▁▂▂▂▂▂▂▁▂▂▂▁▂▁▂</td></tr><tr><td>mean_cosine_similarity_true_pairs</td><td>▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█▇██████</td></tr><tr><td>train_loss</td><td>▆▆▅█▅▅▄▄▄▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>1.3</td></tr><tr><td>backward_r10</td><td>8</td></tr><tr><td>backward_r5</td><td>4.7</td></tr><tr><td>backward_ravg</td><td>4.7</td></tr><tr><td>backward_recall</td><td>1.3/4.7/8.0</td></tr><tr><td>forward_r1</td><td>1.1</td></tr><tr><td>forward_r10</td><td>8.7</td></tr><tr><td>forward_r5</td><td>5.1</td></tr><tr><td>forward_ravg</td><td>5</td></tr><tr><td>gap</td><td>0.1992</td></tr><tr><td>mean_angular_value_image</td><td>0.0251</td></tr><tr><td>mean_angular_value_text</td><td>0.0113</td></tr><tr><td>mean_cosine_similarity_true_pairs</td><td>0.2085</td></tr><tr><td>train_loss</td><td>-0.11942</td></tr><tr><td>uniformity</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP-2025-01-02-21-19-36</strong> at: <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/ntlwxocu' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/ntlwxocu</a><br> View project at: <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 30 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250102_213522-ntlwxocu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%prun\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Minimum configuration (overwrites defaults)\n",
    "    config[\"num_train_samples\"] = -1\n",
    "    config[\"num_test_samples\"] = -1\n",
    "    config[\"evaluate_every_n_batches\"] = 30\n",
    "    config[\"visualize_every_n_batches\"] = 30\n",
    "    config[\"epochs\"] = 1\n",
    "    \n",
    "    \n",
    "    # Baseline\n",
    "    config[\"loss_type\"] = \"anchor\"\n",
    "    print(\"\\nTraining Baseline model\")\n",
    "    main(config)\n",
    "    \n",
    "    \n",
    "    # Anchor + Lunif (HAVE TO FINISH TESTING)\n",
    "    config[\"loss_type\"] = \"anchor+lunif\"\n",
    "    print(\"\\nTraining Anchor + Lunif model\")\n",
    "    main(config)\n",
    "    \n",
    "    # Lunif(50itr)+frozen(text_embed)\n",
    "    # config[\"loss_type\"] = \"lunif(50batch)+frozen(text_embed)\"\n",
    "    # print(\"\\nTraining Lunif(50itr)+frozen(text_embed) model\")\n",
    "    # main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %doctest_mode\\n\\n\\ndef dataset_details():\\n    # Print dataset details\\n    print(\\'Number of samples:\\', len(train_coco)) # 118287 images\\n\\n    # Access a specific sample (4th sample here)\\n    img, target = train_coco[3]  # Load the 4th sample (index 3)\\n\\n    # Display information about the sample\\n    print(\"Image Size:\", img.size())  # Torch tensor size\\n    #plt.imshow(img.permute(1, 2, 0))  # Display the image\\n    print(\"Captions:\", target)  # Captions for the image\\n\\nfor images, captions_list in train_loader:\\n    # images.shape is e.g. (N, 3, 224, 224)\\n    # captions_list has length N, but each item might be a tuple of possible captions\\n\\n    plt.imshow(images[0].permute(1, 2, 0))\\n    plt.show()\\n    plt.imshow(images[1].permute(1, 2, 0))\\n    plt.show()\\n\\n    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\\n    print(\"Captions list length:\", len(captions_list))\\n    \\n    print(\"Captions list:\", list(captions_list))\\n\\n    print(\"Number of chosen captions:\", len(list(captions_list[0])))\\n    \\n    captions = list(captions_list[0])\\n\\n    # Then tokenize\\n    text_tokens = tokenizer.tokenize(captions)\\n    print(\"Text tokens shape:\", text_tokens.shape)\\n\\n    # Now encode\\n    #image_embeds = model.encode_image(images.to(device))\\n    #text_embeds = model.encode_text(text_tokens.to(device))\\n\\n    # Should both be shape (N, D)\\n    #print(\"Image embeds shape:\", image_embeds.shape)\\n    #print(\"Text  embeds shape:\", text_embeds.shape)\\n\\n    break  # just to test one batch\\n    \\n\\ndef collate_fn_debug(batch):\\n    print(\"Bath type:\", type(batch)) # This is a list\\n    print(\"Batch size:\", len(batch))\\n    print(\"Batch:\", batch)\\n    images, captions = zip(*batch)\\n    \\n    print(\"Images type:\", type(images))\\n    print(\"Images size:\", len(images))\\n    print(\"Images:\", images)\\n    \\n    print(\"Captions type:\", type(captions))\\n    print(\"Captions size:\", len(captions))\\n    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\\n    \\n    # Select one caption per image\\n    sel_captions = []\\n    for list_captions in captions:\\n        #print(\"List Captions:\", list_captions)\\n        caption = random.choice(list_captions)\\n        sel_captions.append(caption)\\n    \\n    print(\"Selected Captions:\", sel_captions)    \\n\\n\\n\\nfor images, captions_list in train_loader:\\n    break\\n\\n# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\\n\\n '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
