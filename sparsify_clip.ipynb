{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from open_clip import tokenizer\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "# OpenCLIP imports\n",
    "import open_clip\n",
    "\n",
    "# Path to images and annotations\n",
    "image_dir = './coco/images/train2017/'  # Path to train2017 images\n",
    "annotation_file = './coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "# Define the transform to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # or whatever size your model expects\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "cap = dset.CocoCaptions(\n",
    "    root=image_dir,\n",
    "    annFile=annotation_file,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print('Number of samples:', len(cap)) # 118287 images\n",
    "\n",
    "# Check if every image has a caption\n",
    "for i in range(5000):\n",
    "    img, target = cap[i]\n",
    "    #print(\"Image Size:\", img.size())\n",
    "    if len(target) == 0:\n",
    "        print(\"No caption for image\", i)\n",
    "\n",
    "\n",
    "# Access a specific sample (4th sample here)\n",
    "img, target = cap[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "# Display information about the sample\n",
    "print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64  # example\n",
    "loader = DataLoader(cap, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-B-32\"        # Example architecture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "\n",
    "# Create model & transforms from scratch (no pretrained weights)\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "    model_name,\n",
    "    pretrained=None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Put the model into training mode\n",
    "model.train()\n",
    "\n",
    "# If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings (optional, but typical in CLIP-like models)\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "# Example config\n",
    "lr = 1e-4\n",
    "epochs = 1\n",
    "temperature = 0.07\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, captions_list in tqdm.tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # For COCO, each item can have multiple captions. \n",
    "        # We'll just pick the first caption from each list for this example:\n",
    "        captions = list(captions_list[0])\n",
    "\n",
    "        #print(\"Captions:\", captions)\n",
    "        #print(\"Captions length:\", len(captions))\n",
    "\n",
    "        # Tokenize text (open_clip tokenizer produces tokenized batch)\n",
    "        text_tokens = tokenizer.tokenize(captions)\n",
    "        text_tokens = text_tokens.to(device)\n",
    "\n",
    "        # Encode image and text\n",
    "        image_embeds = model.encode_image(images)\n",
    "        text_embeds  = model.encode_text(text_tokens)\n",
    "\n",
    "        # Compute the contrastive loss\n",
    "        loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, captions_list in loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    print(\"Image batch size:\", \"COMPLETE:\" ,images.shape[0], images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list[0]))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list[0]))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    image_embeds = model.encode_image(images.to(device))\n",
    "    text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "\n",
    "\n",
    "# TODO: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
