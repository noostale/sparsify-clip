{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \n",
    "        \"run_name\":                     \"CLIP-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    "        \"device_id\":                    1,      # GPU id\n",
    "        \"seed\":                         42,     # Random seed\n",
    "        \n",
    "        \"learning_rate\":                1e-4,\n",
    "        \"batch_size\":                   128,\n",
    "        \"epochs\":                       1,\n",
    "        \"model\":                        \"RN50\",\n",
    "        \n",
    "        \"temperature\":                  0.07,\n",
    "        \n",
    "        \"loss_type\":                    \"anchor+lunif\",   # anchor, anchor+lunif\n",
    "        \n",
    "        \"num_train_samples\":            -1,            # -1 for all\n",
    "        \"num_test_samples\":             -1,            # -1 for all\n",
    "        \"evaluate_every_n_batches\":     200,\n",
    "        \"visualize_every_n_batches\":    10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lunif_loss(x, t=2):\n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    # Check that the score matrix has the correct shape\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "    return eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_embeddings(text_embeddings, vision_embeddings, \n",
    "                         sample_size=1000, method='pca', \n",
    "                         title=\"Embeddings Visualization\",\n",
    "                         save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes text and vision embeddings in 2D or 3D using PCA, t-SNE, or UMAP.\n",
    "\n",
    "    Args:\n",
    "        text_embeddings (torch.Tensor): \n",
    "            Shape [N, D] containing text embeddings.\n",
    "        vision_embeddings (torch.Tensor):\n",
    "            Shape [N, D] containing vision/image embeddings.\n",
    "        sample_size (int): \n",
    "            If the embeddings contain more than 'sample_size' samples, \n",
    "            randomly pick this many for faster plotting. Set -1 to use all.\n",
    "        method (str): \n",
    "            \"pca\", \"tsne\", or \"umap\".\n",
    "        title (str): \n",
    "            Title for the plot.\n",
    "        save_path (str, optional): \n",
    "            If provided, saves the plot to this path instead of showing it.\n",
    "    \"\"\"\n",
    "    # Detach from graph and bring to CPU if the tensors require grad\n",
    "    text_np = text_embeddings.detach().cpu().numpy()\n",
    "    vision_np = vision_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Optionally downsample for quicker plotting\n",
    "    if sample_size != -1:\n",
    "        n_text = text_np.shape[0]\n",
    "        n_vision = vision_np.shape[0]\n",
    "\n",
    "        n_samples = min(n_text, n_vision)\n",
    "\n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "            text_np = text_np[indices]\n",
    "            vision_np = vision_np[indices]\n",
    "\n",
    "    # Combine for joint dimensionality reduction\n",
    "    all_data = np.concatenate([text_np, vision_np], axis=0)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=3)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"tsne\":\n",
    "        reducer = TSNE(n_components=3, perplexity=30, max_iter=250, random_state=42)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=3, random_state=42, n_jobs=1)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 'pca', 'tsne', and 'umap' are implemented.\")\n",
    "\n",
    "    # Split back into text and vision\n",
    "    text_reduced = reduced[: len(text_np)]\n",
    "    vision_reduced = reduced[len(text_np):]\n",
    "\n",
    "    # Plot 3D visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(text_reduced[:, 0], text_reduced[:, 1], text_reduced[:, 2], \n",
    "               c='red', alpha=0.6, label='Text')\n",
    "    ax.scatter(vision_reduced[:, 0], vision_reduced[:, 1], vision_reduced[:, 2], \n",
    "               c='blue', alpha=0.6, label='Vision')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        wandb.log({title: wandb.Image(plt)})\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, test_loader, device):\n",
    "\n",
    "    # Create model & transforms from scratch (no pretrained weights) #TODO: Use the tokenizer from the chosen model, not the default one\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "        config[\"model\"],\n",
    "        pretrained=None,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Set up training parameters from the config\n",
    "    lr = config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "\n",
    "    # Move the model to multiple GPUs\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    current_batch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, captions_list in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            current_batch += 1\n",
    "            \n",
    "            # Move data to the primary device\n",
    "            images = images.to(device)\n",
    "            captions = captions_list\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer.tokenize(captions)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Encode image and text\n",
    "            image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "            text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "            \n",
    "            # Compute loss based on the experiment type\n",
    "            if config[\"loss_type\"] == \"anchor\":\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "            elif config[\"loss_type\"] == \"anchor+lunif\":\n",
    "                lunif_img = lunif_loss(image_embeds)\n",
    "                lunif_txt = lunif_loss(text_embeds)\n",
    "                lunif = (lunif_img + lunif_txt) / 2\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif\n",
    "            elif config[\"loss_type\"] == \"lunif(50batch)+frozen(text_embed)\":\n",
    "                if current_batch <= 50:\n",
    "                    lunif_img = lunif_loss(image_embeds)\n",
    "                    lunif_txt = lunif_loss(text_embeds)\n",
    "                    lunif = (lunif_img + lunif_txt) / 2\n",
    "                    loss = lunif\n",
    "                else: # train on anchor loss with frozen text embeddings\n",
    "                    text_embeds = text_embeds.detach()\n",
    "                    loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "                    \n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if current_batch % config[\"visualize_every_n_batches\"] == 0:\n",
    "                 visualize_embeddings(text_embeds, \n",
    "                     image_embeds, \n",
    "                     sample_size=1000, \n",
    "                     method='umap', \n",
    "                     title=\"CLIP Embeddings Visualization\",\n",
    "                     save_path=\"embeddings_plot.png\")\n",
    "            \n",
    "            if current_batch % config[\"evaluate_every_n_batches\"] == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "                print(\"Evaluating model...\")\n",
    "                test_results = evaluate_model(model, test_loader, device)\n",
    "                \n",
    "                wandb.log(test_results)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(config):\n",
    "\n",
    "    # Path to train images and annotations\n",
    "    train_image_dir = './data/coco/images/train2017/'                          # Path to train2017 images\n",
    "    train_annotation_file = './data/coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "    # Path to test (val) images and annotations\n",
    "    test_image_dir = './data/coco/images/val2017/'                          # Path to val2017 images\n",
    "    test_annotation_file = './data/coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "    # Define the transform to be applied to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to the model's required input size\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_coco = dset.CocoCaptions(\n",
    "        root=train_image_dir,\n",
    "        annFile=train_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_coco = dset.CocoCaptions(\n",
    "        root=test_image_dir,\n",
    "        annFile=test_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    if config[\"num_train_samples\"] != -1:\n",
    "        print(f\"Subsetting the training dataset to {config['num_train_samples']} samples\")\n",
    "        # Subset the training dataset\n",
    "        num_training_samples = config[\"num_train_samples\"]\n",
    "        subset_indices = list(range(num_training_samples))\n",
    "        train_coco = Subset(train_coco, subset_indices)\n",
    "    \n",
    "    if config[\"num_test_samples\"] != -1:\n",
    "        print(f\"Subsetting the test dataset to {config['num_test_samples']} samples\")\n",
    "        # Subset the test dataset\n",
    "        num_test_samples = config[\"num_test_samples\"]\n",
    "        subset_indices = list(range(num_test_samples))\n",
    "        test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "    # Every image has 5 captions at max, we need to sample one of them\n",
    "    # Create collate function to sample one caption per image\n",
    "    def collate_fn(batch):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        sel_captions = []\n",
    "        for list_captions in captions:\n",
    "            caption = random.choice(list_captions)\n",
    "            sel_captions.append(caption)\n",
    "        return images, sel_captions\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # NumPy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU random numbers\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU random numbers for a single GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random numbers for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark for deterministic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # Finish any existing W&B runs before starting a new one\n",
    "    wandb.finish()\n",
    "\n",
    "    # Initialize your W&B run\n",
    "    wandb.init(project=\"sparsify-clip\", config=config, name=config[\"run_name\"])\n",
    "    \n",
    "    # Print the config\n",
    "    print(\"Config:\", config)\n",
    "    \n",
    "    # Set the device\n",
    "    device_id = config[\"device_id\"]\n",
    "    device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"\\nLoading the dataset...\")\n",
    "    train_loader, test_loader = dataset_loader(config)\n",
    "    print(\"Dataset loaded.\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(config, train_loader, test_loader, device)\n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    print(\"Final evaluation of the model...\")\n",
    "    final_log = evaluate_model(model, test_loader, device)\n",
    "    print(\"Evaluation complete.\\n\")\n",
    "    \n",
    "    # Save the model and upload it to W&B\n",
    "    #torch.save(model.state_dict(), config[\"run_name\"] + \".pt\")\n",
    "    #wandb.save(config[\"run_name\"] + \".pt\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250102_175205-0oaauv44</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/0oaauv44' target=\"_blank\">CLIP-2025-01-02-17-52-04</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/0oaauv44' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/0oaauv44</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'CLIP-2025-01-02-17-52-04', 'device_id': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 3, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor', 'num_train_samples': -1, 'num_test_samples': -1, 'evaluate_every_n_batches': 50, 'visualize_every_n_batches': 10}\n",
      "\n",
      "Loading the dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded.\n",
      "\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 49/924 [00:31<07:49,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3]  Batch: 50  Loss: 4.59527\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.22it/s]\n",
      "  5%|▌         | 50/924 [00:46<1:14:53,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_recall': '0.1/0.4/0.721', 'forward_ravg': 0.401, 'backward_r1': 0.06, 'backward_recall': '0.1/0.3/0.681', 'backward_ravg': 0.341}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 99/924 [01:15<07:12,  1.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3]  Batch: 100  Loss: 4.40957\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:09<00:00,  4.08it/s]\n",
      " 11%|█         | 100/924 [01:32<1:15:11,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_recall': '0.1/0.5/0.942', 'forward_ravg': 0.514, 'backward_r1': 0.08, 'backward_recall': '0.1/0.5/1.042', 'backward_ravg': 0.554}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 109/924 [01:36<09:38,  1.41it/s]  "
     ]
    }
   ],
   "source": [
    "# %%prun\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Minimum configuration (overwrites defaults)\n",
    "    config[\"num_train_samples\"] = -1\n",
    "    config[\"num_test_samples\"] = -1\n",
    "    config[\"evaluate_every_n_batches\"] = 50\n",
    "    config[\"visualize_every_n_batches\"] = 10\n",
    "    config[\"epochs\"] = 3\n",
    "    \n",
    "    \n",
    "    # Baseline\n",
    "    config[\"loss_type\"] = \"anchor\"\n",
    "    print(\"\\nTraining Baseline model\")\n",
    "    main(config)\n",
    "    \n",
    "    \n",
    "    # Anchor + Lunif\n",
    "    config[\"loss_type\"] = \"anchor+lunif\"\n",
    "    print(\"\\nTraining Anchor + Lunif model\")\n",
    "    main(config)\n",
    "    \n",
    "    # Lunif(50itr)+frozen(text_embed)\n",
    "    # config[\"loss_type\"] = \"lunif(50batch)+frozen(text_embed)\"\n",
    "    # print(\"\\nTraining Lunif(50itr)+frozen(text_embed) model\")\n",
    "    # main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %doctest_mode\\n\\n\\ndef dataset_details():\\n    # Print dataset details\\n    print(\\'Number of samples:\\', len(train_coco)) # 118287 images\\n\\n    # Access a specific sample (4th sample here)\\n    img, target = train_coco[3]  # Load the 4th sample (index 3)\\n\\n    # Display information about the sample\\n    print(\"Image Size:\", img.size())  # Torch tensor size\\n    #plt.imshow(img.permute(1, 2, 0))  # Display the image\\n    print(\"Captions:\", target)  # Captions for the image\\n\\nfor images, captions_list in train_loader:\\n    # images.shape is e.g. (N, 3, 224, 224)\\n    # captions_list has length N, but each item might be a tuple of possible captions\\n\\n    plt.imshow(images[0].permute(1, 2, 0))\\n    plt.show()\\n    plt.imshow(images[1].permute(1, 2, 0))\\n    plt.show()\\n\\n    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\\n    print(\"Captions list length:\", len(captions_list))\\n    \\n    print(\"Captions list:\", list(captions_list))\\n\\n    print(\"Number of chosen captions:\", len(list(captions_list[0])))\\n    \\n    captions = list(captions_list[0])\\n\\n    # Then tokenize\\n    text_tokens = tokenizer.tokenize(captions)\\n    print(\"Text tokens shape:\", text_tokens.shape)\\n\\n    # Now encode\\n    #image_embeds = model.encode_image(images.to(device))\\n    #text_embeds = model.encode_text(text_tokens.to(device))\\n\\n    # Should both be shape (N, D)\\n    #print(\"Image embeds shape:\", image_embeds.shape)\\n    #print(\"Text  embeds shape:\", text_embeds.shape)\\n\\n    break  # just to test one batch\\n    \\n\\ndef collate_fn_debug(batch):\\n    print(\"Bath type:\", type(batch)) # This is a list\\n    print(\"Batch size:\", len(batch))\\n    print(\"Batch:\", batch)\\n    images, captions = zip(*batch)\\n    \\n    print(\"Images type:\", type(images))\\n    print(\"Images size:\", len(images))\\n    print(\"Images:\", images)\\n    \\n    print(\"Captions type:\", type(captions))\\n    print(\"Captions size:\", len(captions))\\n    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\\n    \\n    # Select one caption per image\\n    sel_captions = []\\n    for list_captions in captions:\\n        #print(\"List Captions:\", list_captions)\\n        caption = random.choice(list_captions)\\n        sel_captions.append(caption)\\n    \\n    print(\"Selected Captions:\", sel_captions)    \\n\\n\\n\\nfor images, captions_list in train_loader:\\n    break\\n\\n# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\\n\\n '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
