{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples: 1000\n",
      "Image Size: torch.Size([3, 224, 224])\n",
      "Captions: ['A zebra grazing on lush green grass in a field.', 'Zebra reaching its head down to ground where grass is. ', 'The zebra is eating grass in the sun.', 'A lone zebra grazing in some green grass.', 'a Zebra grazing on grass in a green open field.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "# OpenCLIP imports\n",
    "import open_clip\n",
    "\n",
    "# Path to train images and annotations\n",
    "train_image_dir = './coco/images/train2017/'  # Path to train2017 images\n",
    "train_annotation_file = './coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "# Path to test (val) images and annotations\n",
    "test_image_dir = './coco/images/val2017/'  # Path to val2017 images\n",
    "test_annotation_file = './coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "# Define the transform to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # or whatever size your model expects\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the training dataset\n",
    "train_coco = dset.CocoCaptions(\n",
    "    root=train_image_dir,\n",
    "    annFile=train_annotation_file,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create the test dataset\n",
    "test_coco = dset.CocoCaptions(\n",
    "    root=test_image_dir,\n",
    "    annFile=test_annotation_file,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "num_samples = 1000\n",
    "subset_indices = list(range(num_samples))\n",
    "train_coco = Subset(train_coco, subset_indices)\n",
    "\n",
    "\n",
    "# Print dataset details\n",
    "print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "# Access a specific sample (4th sample here)\n",
    "img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "# Display information about the sample\n",
    "print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "#plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "# Every image has 5 captions at max, we need to sample one of them\n",
    "# Create collate function to sample one caption per image\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    return images, sel_captions\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_coco, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch size: 128 Shape: torch.Size([128, 3, 224, 224])\n",
      "Captions list length: 128\n",
      "Captions list: ['A man in an office chair looking at a laptop next to a glass of wine.', 'Hot dogs, buns, and croissants on grill with red wall', 'A boat motors past a man playing Frisbee on the beach ', 'An individual is capture in the stillness of the picture.\\n', 'Children on beright sunny day playing soccer who appear to be about 5 years old. ', 'Man standing in an auditorium filled with chairs.', 'a living room with a big colorful rug on the floor ', 'A woman kneeling down on top of a baseball field.', 'a small kid looks up at a kite ', 'Gazelles, zebras and giraffes roaming around the plains.', 'Assorted fruit are laying on a cutting board.', 'There is a little toy hanging on the key chain. ', 'A man smiles wearing beads and a neck tie.', 'Fresh produce has been harvested to take to market.', 'Three people standing outsidde a mall in the fall.', 'a very tall giraffe walking in a bush with his neck high', 'A close up of a layered dessert on top of a paper towel and plate.', 'a woman about to hit a volley ball during a game', 'A red fire hydrant and a sign next to tree', 'Pottery is displayed on tables in the room where a man is.', 'Four giraffes standing among the trees and brush.', 'The woman in black pants stands holding a video game remote.', 'A top of a persons head and a kite in the distance. ', 'cars are parked in the parking spaces with meters', 'Several yellow bananas are growing on the plant.', 'a sink with a green countertop next to a white wall ', 'a person is performing a snowboard trick on the hill', 'A bog white semi truck with a big grill protector.', 'Flash from camera glares in the mirror above the toilet', 'Three oranges in a smooth wood bowl on a counter.', 'A close-up of a zebra in the wild.', 'two people siting on a curb with a cell phone ', 'A large and furnished green walled apartment livingroom', 'A couple of horses are leading the parade like float. ', 'A young man touches the bottom of his tie.', 'Stuffed sandwich with food items shown in outdoor setting.', 'a man is skateboarding on the edge of a building', 'A goat in a field near a white and green church', 'A gray TV sitting on top of a TV shelf.', 'A group of sheep grazing grass on a large field.', 'a brown bear has its mouth open and some trees', 'A surfer riding a wave in the ocean.', 'A fire hydrant with a spout and handle, with frozen water under the spout.', 'A man standing in a kitchen preparing food.', 'Two small ponies trot across the grass in a pen.', 'an image of a bird perched on an ornament', 'a gathering of a group of people at the park', 'A pepperoni pizza with black olives and cheese ', \"A zebra walking around it's habitat at a zoo.\", 'A woman holding a cell phone to her ear. ', 'A couple of large giraffe in a field.', 'A bed is shown with pillows and a towel.', 'A surfer is carrying his board into the water.', 'A man in white jacket skiing up snowy slope.', 'Boats float in a lake in front of a grassy area.', 'A person sitting in the dark by some computers.', 'A dirty brown teddy bear in a trash can.', 'A brightly lit living area with a speaker system', 'Three people are going snow boarding in the cold.', 'A bathroom with safety bars and a toilet.', 'Four zebras are grazing in a grassy enclosure.', 'Man in red shirt taking care of a brown horse.', 'Large platter of cooked meat is displayed on a table.', 'a toilet sitting on a tiled floor in enclosed bathroom stall', 'a blue piggy back passenger bus parked next to a big building', 'An empty subway train at an empty station.', \"A cat sticking it's head in a pot.\", 'A row of boys are on mini bikes on a dirt track.', 'The toilet is enclosed in the shower in this small bathroom.', 'Pink covered bed and table in crowded bedroom.', 'A skate boarder is doing a jump over a patch in side walk.', 'A bed with pirate bedding that is red white and blue', 'A LARGE KITE LEANING AGAINST A TABLE WITH A MAN IN THE BACKGROUND ', 'a street with a few cars and some leaves in it ', 'A television tuned in to a news alert.', 'Two workers are heading down the road on their horses. ', 'a desk with two keyboards a mouse and a monitor', 'A woman sitting on a chair with a spoon in her hand.', 'Group of zebras grazing in dry grassy plain.', 'a person holding a bottle of wine in front of a black stove.', 'PEOPLE ARE RDIIDNG ON THE BACK OF AN ELEPHANT', 'A white plate topped with a sandwich filled with meat and fries.', 'Many boats floating on the ocean next to a sandy shore.', 'A man on a mirror holding his head', 'a bowl of soup some bread and a glass of wine sitting on a table', 'a man drinking from a glass while sitting in front of a table full of food', 'Several kites in the sky with vehicles and people in the background.', 'A person preparing to throw a Frisbee while others watch.', 'A man power sliding on a long board ', 'a close up of a person kissing another person ', 'A bunch of luggage that is sitting on the ground and a couch.', 'A bird sitting below a cross under an arch.', 'two people holding open umbrellas near one another ', 'A man standing next to a news stand on a street.', 'Refueling truck near commercial airliner on tarmac on cloudy day.', 'A woman walking down a street next to shops.', 'a close up of a person holding a bowl of sauce', 'A bathroom with sink and toilet shown in the picture ', 'A cluttered desk with several computers on it', 'A couple of books are placed on the bed. ', 'a wooden table with a mirror and a mass of cut hair on white papers next to a pair of blue handled scissors.', 'A PINK TOILET AND A PINK BATH TUB IN THE BATHROOM', 'Three men that are sitting on a couch.', 'A view from very high up in a plane looking down.', 'A bed sitting in a bedroom next to a window.', 'A young man on a skateboard in a park', 'a bowl of vegetables decorated with small purple flowers', 'a brown and black tower with windows and a white clock ', 'A long train is by some grass and some trees. ', 'A giraffe stands near some large rocks and trees.', 'Tiled bathroom stall with two red buckets on floor.', 'A man performing on stage with various speakers and microphones. ', 'A truck is sitting parked on a road.', 'A man riding a bike down a street past a young man.', 'A man sitting on a couch in a hotel room that also has a bed and desk.', 'A plate full of bread, lettuce and vegetables.  ', 'A group of kids playing in the field', 'Woman and two men riding on a horse drawn chariot.', 'Two teddy bears; one dressed as a female and one male ', 'A plate filled with pastry desserts on it. ', 'A table with a fancy decorated cake and candles in blue and white.', 'Woman on a tennis court with feet off the ground.', 'a group of horses standing around a grassy field under a grey sky', 'A group of people around a birthday cake.', 'A yellow vase of purple flowers and green stems.', 'an older male in a gray sweater a knife and cake', 'A cream colored living room with a cathedral ceiling.', 'A small boat floating out in the middle of a body of water.']\n",
      "Number of chosen captions: 69\n",
      "Text tokens shape: torch.Size([69, 77])\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:08<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Loss: 4.8527\n"
     ]
    }
   ],
   "source": [
    "# %%prun to profile and see where the time is spent\n",
    "\n",
    "model_name = \"ViT-B-32\"        # Example architecture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "\n",
    "# Create model & transforms from scratch (no pretrained weights)\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "    model_name,\n",
    "    pretrained=None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Put the model into training mode\n",
    "model.train()\n",
    "\n",
    "# If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings (optional, but typical in CLIP-like models)\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example config\n",
    "lr = 1e-4\n",
    "epochs = 1\n",
    "temperature = 0.07\n",
    "\n",
    "# Move the model to multiple GPUs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "current_batch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, captions_list in tqdm.tqdm(train_loader):\n",
    "        \n",
    "        current_batch += 1\n",
    "        \n",
    "        # Move data to the primary device\n",
    "        images = images.to(device)\n",
    "        captions = captions_list\n",
    "\n",
    "        # Tokenize text\n",
    "        text_tokens = tokenizer.tokenize(captions)\n",
    "        text_tokens = text_tokens.to(device)\n",
    "\n",
    "        # Encode image and text\n",
    "        image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "        text_embeds = model.module.encode_text(text_tokens)\n",
    "\n",
    "        # Compute the contrastive loss\n",
    "        loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
