{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 1,\n",
    "        \"model\": \"RN50\",\n",
    "        \n",
    "        \"temperature\": 0.07,\n",
    "        \n",
    "        \"num_train_samples\": 50000,\n",
    "        \"num_test_samples\": 5000,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>▁▁▁▁</td></tr><tr><td>backward_ravg</td><td>▁▁█▃</td></tr><tr><td>forward_r1</td><td>▁▁▁▁</td></tr><tr><td>forward_ravg</td><td>▁▁▁█</td></tr><tr><td>train_loss</td><td>▄▄█▁▆▄▃▃▃▅▇▇▇▇▇▇▇▇▇▇▆▇▆▆▆▆▆▆▆▆▆▅▇▇▇▇▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>backward_r1</td><td>0.02</td></tr><tr><td>backward_ravg</td><td>0.127</td></tr><tr><td>backward_recall</td><td>0.0/0.1/0.22</td></tr><tr><td>forward_r1</td><td>0.02</td></tr><tr><td>forward_ravg</td><td>0.113</td></tr><tr><td>forward_recall</td><td>0.0/0.1/0.22</td></tr><tr><td>train_loss</td><td>1.29571</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP-2024-12-31-18-17-19</strong> at: <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/9pbxlzkj' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/9pbxlzkj</a><br> View project at: <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241231_181720-9pbxlzkj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20241231_182249-wcfv64p4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/wcfv64p4' target=\"_blank\">CLIP-2024-12-31-18-22-49</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/wcfv64p4' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/wcfv64p4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from open_clip import tokenizer\n",
    "from torch.utils.data import Subset\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Initialize your W&B run\n",
    "wandb.init(\n",
    "    project=\"sparsify-clip\",  # The name of your project\n",
    "    config=config,\n",
    "    name=\"CLIP-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    ")\n",
    "\n",
    "# Path to train images and annotations\n",
    "train_image_dir = './coco/images/train2017/'                          # Path to train2017 images\n",
    "train_annotation_file = './coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "# Path to test (val) images and annotations\n",
    "test_image_dir = './coco/images/val2017/'                          # Path to val2017 images\n",
    "test_annotation_file = './coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "# Define the transform to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # or whatever size your model expects\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the training dataset\n",
    "train_coco = dset.CocoCaptions(\n",
    "    root=train_image_dir,\n",
    "    annFile=train_annotation_file,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create the test dataset\n",
    "test_coco = dset.CocoCaptions(\n",
    "    root=test_image_dir,\n",
    "    annFile=test_annotation_file,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Subset the training dataset\n",
    "num_training_samples = config[\"num_train_samples\"]\n",
    "subset_indices = list(range(num_training_samples))\n",
    "train_coco = Subset(train_coco, subset_indices)\n",
    "\n",
    "# Subset the test dataset\n",
    "num_test_samples = config[\"num_test_samples\"]\n",
    "subset_indices = list(range(num_test_samples))\n",
    "test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "# Every image has 5 captions at max, we need to sample one of them\n",
    "# Create collate function to sample one caption per image\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    return images, sel_captions\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = config[\"batch_size\"]\n",
    "train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings (optional, but typical in CLIP-like models)\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lunif_loss(x, t=2):\n",
    "    \n",
    "    x = F.normalize(x, dim=-1)\n",
    "        \n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get batch sizes\n",
    "    batch_size1 = text_embeddings.shape[0]   # For text embeddings\n",
    "    batch_size2 = visual_embeddings.shape[0]  # For visual embeddings\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    print(len(ids_txt),len(ids))\n",
    "    print(score_matrix.shape)\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "\n",
    "    return eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    \"\"\"\n",
    "\n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 19/390 [00:22<07:10,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 20  Loss: 3.48998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/390 [01:11<1:35:13, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.04, 'forward_recall': '0.0/0.1/0.14', 'forward_ravg': 0.08, 'backward_r1': 0.02, 'backward_recall': '0.0/0.1/0.22', 'backward_ravg': 0.114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 39/390 [01:35<07:33,  1.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 40  Loss: 3.90224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 40/390 [02:24<1:31:41, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.02, 'forward_recall': '0.0/0.3/0.521', 'forward_ravg': 0.267, 'backward_r1': 0.04, 'backward_recall': '0.0/0.2/0.501', 'backward_ravg': 0.254}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 59/390 [02:47<06:27,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 60  Loss: 3.51593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 60/390 [03:36<1:25:21, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_recall': '0.1/0.3/0.601', 'forward_ravg': 0.347, 'backward_r1': 0.08, 'backward_recall': '0.1/0.3/0.541', 'backward_ravg': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 79/390 [03:57<05:59,  1.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 80  Loss: 3.45159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:44<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 80/390 [04:47<1:20:46, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.08, 'forward_recall': '0.1/0.6/0.942', 'forward_ravg': 0.534, 'backward_r1': 0.12, 'backward_recall': '0.1/0.4/0.701', 'backward_ravg': 0.407}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 99/390 [05:08<05:35,  1.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 100  Loss: 3.31263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 100/390 [05:57<1:14:33, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.1, 'forward_recall': '0.1/0.6/0.921', 'forward_ravg': 0.528, 'backward_r1': 0.06, 'backward_recall': '0.1/0.4/0.781', 'backward_ravg': 0.414}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 119/390 [06:19<05:13,  1.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 120  Loss: 3.08761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 120/390 [07:08<1:09:05, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.02, 'forward_recall': '0.0/0.7/1.182', 'forward_ravg': 0.641, 'backward_r1': 0.1, 'backward_recall': '0.1/0.6/0.921', 'backward_ravg': 0.534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 139/390 [07:30<04:54,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 140  Loss: 3.07114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 140/390 [08:19<1:05:01, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.18, 'forward_recall': '0.2/0.7/1.142', 'forward_ravg': 0.674, 'backward_r1': 0.18, 'backward_recall': '0.2/0.5/0.841', 'backward_ravg': 0.507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 159/390 [08:41<04:31,  1.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 160  Loss: 2.97673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 160/390 [09:30<59:43, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.08, 'forward_recall': '0.1/0.6/1.242', 'forward_ravg': 0.648, 'backward_r1': 0.04, 'backward_recall': '0.0/0.7/1.242', 'backward_ravg': 0.654}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 179/390 [09:52<04:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 180  Loss: 2.93536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 180/390 [10:41<54:17, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.12, 'forward_recall': '0.1/0.7/1.382', 'forward_ravg': 0.741, 'backward_r1': 0.12, 'backward_recall': '0.1/0.5/1.062', 'backward_ravg': 0.554}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 199/390 [11:03<03:44,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 200  Loss: 3.06189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 200/390 [11:52<48:53, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.14, 'forward_recall': '0.1/0.9/1.663', 'forward_ravg': 0.901, 'backward_r1': 0.2, 'backward_recall': '0.2/0.5/1.082', 'backward_ravg': 0.601}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 219/390 [12:14<03:17,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 220  Loss: 2.74465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:45<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 220/390 [13:05<45:46, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.16, 'forward_recall': '0.2/0.8/1.402', 'forward_ravg': 0.781, 'backward_r1': 0.2, 'backward_recall': '0.2/0.9/1.663', 'backward_ravg': 0.915}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 239/390 [13:27<02:59,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 240  Loss: 2.77369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 240/390 [14:16<38:46, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_recall': '0.3/1.0/1.683', 'forward_ravg': 0.982, 'backward_r1': 0.18, 'backward_recall': '0.2/0.8/1.623', 'backward_ravg': 0.868}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 259/390 [14:38<02:36,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 260  Loss: 2.67248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 260/390 [15:27<33:30, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.18, 'forward_recall': '0.2/1.0/1.803', 'forward_ravg': 1.002, 'backward_r1': 0.26, 'backward_recall': '0.3/0.9/1.863', 'backward_ravg': 1.022}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 279/390 [15:49<02:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 280  Loss: 2.79094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:44<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 280/390 [16:39<28:57, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.16, 'forward_recall': '0.2/1.1/1.903', 'forward_ravg': 1.042, 'backward_r1': 0.26, 'backward_recall': '0.3/0.9/1.863', 'backward_ravg': 1.015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 299/390 [17:01<01:45,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 300  Loss: 2.48632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 300/390 [17:51<23:34, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.321, 'forward_recall': '0.3/1.3/2.444', 'forward_ravg': 1.349, 'backward_r1': 0.321, 'backward_recall': '0.3/1.0/1.843', 'backward_ravg': 1.062}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 319/390 [18:13<01:24,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 320  Loss: 2.51419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 320/390 [19:03<18:25, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.3, 'forward_recall': '0.3/1.4/2.143', 'forward_ravg': 1.289, 'backward_r1': 0.22, 'backward_recall': '0.2/1.4/2.284', 'backward_ravg': 1.309}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 339/390 [20:07<04:00,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 340  Loss: 2.48775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [02:05<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 340/390 [22:22<36:24, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.28, 'forward_recall': '0.3/1.4/2.544', 'forward_ravg': 1.409, 'backward_r1': 0.3, 'backward_recall': '0.3/1.4/2.544', 'backward_ravg': 1.409}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 359/390 [22:48<00:42,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 360  Loss: 2.44322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [01:21<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 360/390 [24:16<13:35, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.341, 'forward_recall': '0.3/1.7/3.285', 'forward_ravg': 1.776, 'backward_r1': 0.361, 'backward_recall': '0.4/1.6/2.945', 'backward_ravg': 1.636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 379/390 [24:38<00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Batch: 380  Loss: 2.36717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:44<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 380/390 [25:28<02:38, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'forward_r1': 0.401, 'forward_recall': '0.4/1.7/3.285', 'forward_ravg': 1.81, 'backward_r1': 0.361, 'backward_recall': '0.4/1.6/3.025', 'backward_ravg': 1.649}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [25:39<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1]  Loss: 2.2905\n"
     ]
    }
   ],
   "source": [
    "# %%prun to profile and see where the time is spent\n",
    "\n",
    "\n",
    "#model_name = \"ViT-B-32\"        # Example architecture\n",
    "model_name = config[\"model\"]    # Example architecture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "\n",
    "# Create model & transforms from scratch (no pretrained weights)\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "    model_name,\n",
    "    pretrained=None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Put the model into training mode\n",
    "model.train()\n",
    "\n",
    "# If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Example config\n",
    "lr = config[\"learning_rate\"]\n",
    "epochs = config[\"epochs\"]\n",
    "temperature = config[\"temperature\"]\n",
    "\n",
    "# Move the model to multiple GPUs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "current_batch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, captions_list in tqdm.tqdm(train_loader):\n",
    "        \n",
    "        current_batch += 1\n",
    "        \n",
    "        # Move data to the primary device\n",
    "        images = images.to(device)\n",
    "        captions = captions_list\n",
    "\n",
    "        # Tokenize text\n",
    "        text_tokens = tokenizer.tokenize(captions)\n",
    "        text_tokens = text_tokens.to(device)\n",
    "\n",
    "        # Encode image and text\n",
    "        image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "        text_embeds = model.module.encode_text(text_tokens)\n",
    "        \n",
    "        \n",
    "        # Coompute the lunif loss\n",
    "        lunif_img = lunif_loss(image_embeds)\n",
    "        lunif_txt = lunif_loss(text_embeds)\n",
    "        lunif = (lunif_img + lunif_txt) / 2\n",
    "        \n",
    "\n",
    "        # Compute the contrastive loss\n",
    "        loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif / 2\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if current_batch % 20 == 0:\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "            test_results = evaluate_model(model, test_loader, device)\n",
    "            \n",
    "            wandb.log(test_results)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "4992 4992\n",
      "torch.Size([4992, 4992])\n",
      "Evaluation Results: {'forward_r1': 0.421, 'forward_recall': '0.4/1.6/2.945', 'forward_ravg': 1.669, 'backward_r1': 0.341, 'backward_recall': '0.3/1.2/2.444', 'backward_ravg': 1.329}\n"
     ]
    }
   ],
   "source": [
    "final_log = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# 3) final_log now contains your forward/backward R@1, R@5, R@10 metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %doctest_mode\\n\\n\\ndef dataset_details():\\n    # Print dataset details\\n    print(\\'Number of samples:\\', len(train_coco)) # 118287 images\\n\\n    # Access a specific sample (4th sample here)\\n    img, target = train_coco[3]  # Load the 4th sample (index 3)\\n\\n    # Display information about the sample\\n    print(\"Image Size:\", img.size())  # Torch tensor size\\n    #plt.imshow(img.permute(1, 2, 0))  # Display the image\\n    print(\"Captions:\", target)  # Captions for the image\\n\\nfor images, captions_list in train_loader:\\n    # images.shape is e.g. (N, 3, 224, 224)\\n    # captions_list has length N, but each item might be a tuple of possible captions\\n\\n    plt.imshow(images[0].permute(1, 2, 0))\\n    plt.show()\\n    plt.imshow(images[1].permute(1, 2, 0))\\n    plt.show()\\n\\n    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\\n    print(\"Captions list length:\", len(captions_list))\\n    \\n    print(\"Captions list:\", list(captions_list))\\n\\n    print(\"Number of chosen captions:\", len(list(captions_list[0])))\\n    \\n    captions = list(captions_list[0])\\n\\n    # Then tokenize\\n    text_tokens = tokenizer.tokenize(captions)\\n    print(\"Text tokens shape:\", text_tokens.shape)\\n\\n    # Now encode\\n    #image_embeds = model.encode_image(images.to(device))\\n    #text_embeds = model.encode_text(text_tokens.to(device))\\n\\n    # Should both be shape (N, D)\\n    #print(\"Image embeds shape:\", image_embeds.shape)\\n    #print(\"Text  embeds shape:\", text_embeds.shape)\\n\\n    break  # just to test one batch\\n    \\n\\ndef collate_fn_debug(batch):\\n    print(\"Bath type:\", type(batch)) # This is a list\\n    print(\"Batch size:\", len(batch))\\n    print(\"Batch:\", batch)\\n    images, captions = zip(*batch)\\n    \\n    print(\"Images type:\", type(images))\\n    print(\"Images size:\", len(images))\\n    print(\"Images:\", images)\\n    \\n    print(\"Captions type:\", type(captions))\\n    print(\"Captions size:\", len(captions))\\n    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\\n    \\n    # Select one caption per image\\n    sel_captions = []\\n    for list_captions in captions:\\n        #print(\"List Captions:\", list_captions)\\n        caption = random.choice(list_captions)\\n        sel_captions.append(caption)\\n    \\n    print(\"Selected Captions:\", sel_captions)    \\n\\n\\n\\nfor images, captions_list in train_loader:\\n    break\\n\\n# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\\n\\n '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
