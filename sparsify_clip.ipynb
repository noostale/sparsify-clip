{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import tokenizer # To be substituted with the one of chosen model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "from typing import List, Dict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import datetime\n",
    "import open_clip\n",
    "import math\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"\n",
    "    image_embeds: (batch_size, embed_dim)\n",
    "    text_embeds: (batch_size, embed_dim)\n",
    "    temperature: scalar float for scaling similarities\n",
    "    returns: scalar loss (contrastive)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Similarity matrix, shape (bs, bs)\n",
    "    logits = image_embeds @ text_embeds.t()\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Targets are just the diagonal (i.e. 0->0, 1->1, ...)\n",
    "    batch_size = image_embeds.size(0)\n",
    "    target = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    # CE loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits, target)\n",
    "    # CE loss for text->image\n",
    "    loss_t2i = F.cross_entropy(logits.t(), target)\n",
    "\n",
    "    # Average the two directions\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "def lunif_loss(x, t=2):\n",
    "    # Compute pairwise distances between all embeddings\n",
    "    sq_pdist = torch.pdist(x, p=2).pow(2)\n",
    "    \n",
    "    # Apply the uniformity loss formula\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(text_embeddings, vision_embeddings, \n",
    "                         sample_size=1000, method='pca', \n",
    "                         title=\"Embeddings Visualization\",\n",
    "                         save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes text and vision embeddings in 2D or 3D using PCA, t-SNE, or UMAP.\n",
    "\n",
    "    Args:\n",
    "        text_embeddings (torch.Tensor): \n",
    "            Shape [N, D] containing text embeddings.\n",
    "        vision_embeddings (torch.Tensor):\n",
    "            Shape [N, D] containing vision/image embeddings.\n",
    "        sample_size (int): \n",
    "            If the embeddings contain more than 'sample_size' samples, \n",
    "            randomly pick this many for faster plotting. Set -1 to use all.\n",
    "        method (str): \n",
    "            \"pca\", \"tsne\", or \"umap\".\n",
    "        title (str): \n",
    "            Title for the plot.\n",
    "        save_path (str, optional): \n",
    "            If provided, saves the plot to this path instead of showing it.\n",
    "    \"\"\"\n",
    "    # Detach from graph and bring to CPU if the tensors require grad\n",
    "    text_np = text_embeddings.detach().cpu().numpy()\n",
    "    vision_np = vision_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Optionally downsample for quicker plotting\n",
    "    if sample_size != -1:\n",
    "        n_text = text_np.shape[0]\n",
    "        n_vision = vision_np.shape[0]\n",
    "\n",
    "        n_samples = min(n_text, n_vision)\n",
    "\n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
    "            text_np = text_np[indices]\n",
    "            vision_np = vision_np[indices]\n",
    "\n",
    "    # Combine for joint dimensionality reduction\n",
    "    all_data = np.concatenate([text_np, vision_np], axis=0)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=3)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"tsne\":\n",
    "        reducer = TSNE(n_components=3, perplexity=30, max_iter=1000, random_state=42)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    elif method.lower() == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=3, random_state=42, n_jobs=1)\n",
    "        reduced = reducer.fit_transform(all_data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 'pca', 'tsne', and 'umap' are implemented.\")\n",
    "\n",
    "    # Split back into text and vision\n",
    "    text_reduced = reduced[: len(text_np)]\n",
    "    vision_reduced = reduced[len(text_np):]\n",
    "\n",
    "    # Plot 3D visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(text_reduced[:, 0], text_reduced[:, 1], text_reduced[:, 2], \n",
    "               c='red', alpha=0.6, label='Text')\n",
    "    ax.scatter(vision_reduced[:, 0], vision_reduced[:, 1], vision_reduced[:, 2], \n",
    "               c='blue', alpha=0.6, label='Vision')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        wandb.log({method: wandb.Image(save_path)})\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(text_embeddings, visual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each pair of samples between text embeddings and visual embeddings\n",
    "    by calculating the mean of the corresponding feature vectors across the two modalities.\n",
    "\n",
    "    Parameters:\n",
    "    - text_embeddings (torch.Tensor): Tensor of shape (batch_size1, feature_dim) representing text embeddings.\n",
    "    - visual_embeddings (torch.Tensor): Tensor of shape (batch_size2, feature_dim) representing visual embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (batch_size1, batch_size2, feature_dim) representing the centroid for each pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute centroids by averaging text and visual embeddings\n",
    "    # Expand the dimensions to allow pairwise computation\n",
    "    text_expanded = text_embeddings.unsqueeze(1)  # Shape: [batch_size1, 1, feature_dim]\n",
    "    visual_expanded = visual_embeddings.unsqueeze(0)  # Shape: [1, batch_size2, feature_dim]\n",
    "\n",
    "    # Compute the centroid by averaging the embeddings\n",
    "    centroids = (text_expanded + visual_expanded) / 2.0\n",
    "\n",
    "    # Compute norms of the centroids\n",
    "    centroid_norms = torch.norm(centroids, dim=-1)\n",
    "\n",
    "    return centroid_norms, centroids\n",
    "\n",
    "def compute_metric_ret(score_matrix: torch.Tensor, ids: List[int], ids_txt: List[int], direction: str = 'forward') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics for either text-to-vision or vision-to-text retrieval.\n",
    "\n",
    "    Args:\n",
    "        score_matrix (torch.Tensor): Similarity matrix of shape [N_text, N_image].\n",
    "        ids (List[int]): List of image IDs.\n",
    "        ids_txt (List[int]): List of text IDs corresponding to images.\n",
    "        direction (str): 'forward' for text-to-vision, 'backward' for vision-to-text.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing retrieval metrics.\n",
    "    \"\"\"\n",
    "    assert score_matrix.shape == (len(ids_txt), len(ids)), f\"Score matrix shape {score_matrix.shape} does not match (len(ids_txt), len(ids))\"\n",
    "\n",
    "    if direction == 'forward':  # Text-to-Vision Retrieval\n",
    "        # Sort each row in descending order\n",
    "        indice_matrix = score_matrix.sort(dim=-1, descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "\n",
    "        rank = torch.tensor(rank).to(score_matrix.device)\n",
    "\n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "\n",
    "        eval_log = {\n",
    "            'forward_r1': round(vr_r1 * 100, 4),\n",
    "            'forward_r5': round(vr_r5 * 100, 4),\n",
    "            'forward_r10': round(vr_r10 * 100, 4),\n",
    "            #'forward_recall': f'{round(vr_r1 * 100, 1)}/{round(vr_r5 * 100, 1)}/{round(vr_r10 * 100, 1)}',\n",
    "            'forward_ravg': round((vr_r1 + vr_r5 + vr_r10) / 3 * 100, 4)\n",
    "        }\n",
    "\n",
    "    else:  # Vision-to-Text Retrieval\n",
    "        # Sort each column in descending order\n",
    "        indice_matrix = score_matrix.sort(dim=0, descending=True)[1].permute(1, 0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices = [idx for idx, id_txt in enumerate(ids_txt) if id_txt == ids[i]]\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "\n",
    "        rank = torch.tensor(rank).to(score_matrix.device)\n",
    "\n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "\n",
    "        eval_log = {\n",
    "            'backward_r1': round(tr_r1 * 100, 4),\n",
    "            'backward_r5': round(tr_r5 * 100, 4),\n",
    "            'backward_r10': round(tr_r10 * 100, 4),\n",
    "            #'backward_recall': f'{round(tr_r1 * 100,1)}/{round(tr_r5 * 100,1)}/{round(tr_r10 * 100,1)}',\n",
    "            'backward_ravg': round((tr_r1 + tr_r5 + tr_r10) / 3 * 100, 4)\n",
    "        }\n",
    "\n",
    "    return eval_log\n",
    "\n",
    "def compute_gap(feat_modality1: torch.Tensor, feat_modality2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between the centroids of two modalities.\n",
    "\n",
    "    Args:\n",
    "        feat_modality1 (torch.Tensor): Feature matrix of modality 1 with shape [N, D].\n",
    "        feat_modality2 (torch.Tensor): Feature matrix of modality 2 with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Euclidean distance between centroids.\n",
    "    \"\"\"\n",
    "    # Ensure features are normalized if required\n",
    "    modality1_centroid = torch.mean(feat_modality1, dim=0)\n",
    "    modality2_centroid = torch.mean(feat_modality2, dim=0)\n",
    "\n",
    "    gap = modality1_centroid - modality2_centroid\n",
    "    norm_gap = torch.norm(gap).item()\n",
    "\n",
    "    return norm_gap\n",
    "\n",
    "def compute_mean_angular_value_of_a_modality(feat_modality: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean angular value (mean cosine similarity) of a modality.\n",
    "\n",
    "    Args:\n",
    "        feat_modality (torch.Tensor): Feature matrix with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Mean angular value.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = feat_modality @ feat_modality.T\n",
    "\n",
    "    # Exclude diagonal elements by creating a mask\n",
    "    mask = ~torch.eye(cos_sim.size(0), dtype=torch.bool, device=cos_sim.device)\n",
    "    cos_sim_no_diag = cos_sim[mask]\n",
    "\n",
    "    mean_cos_sim = cos_sim_no_diag.mean().item()\n",
    "\n",
    "    return mean_cos_sim\n",
    "\n",
    "def uniformity(features_modality1: torch.Tensor, features_modality2: torch.Tensor) -> float:\n",
    "    x = torch.cat([features_modality1, features_modality2], dim=0)\n",
    "    N = x.size(0)\n",
    "    dim = x.size(1)\n",
    "\n",
    "    x_center = torch.mean(x, dim=0, keepdim=True)\n",
    "    covariance = torch.mm((x - x_center).t(), x - x_center) / N\n",
    "\n",
    "    mean =  x.mean(0)\n",
    "    np_mean = mean.data.cpu().numpy()\n",
    "    np_covariance = covariance.data.cpu().numpy()\n",
    "   \n",
    "    ##calculation of part1\n",
    "    part1 = np.sum(np.multiply(np_mean, np_mean))\n",
    "\n",
    "    ##calculation of part2\n",
    "    eps = 1e-8 \n",
    "    S, Q = np.linalg.eig(np_covariance)\n",
    "    S = S + eps\n",
    "\n",
    "    mS = np.sqrt(np.diag(S.clip(min=0)))\n",
    "\n",
    "    covariance_2 = np.dot(np.dot(Q, mS), Q.T)\n",
    "\n",
    "    part2 = np.trace(np_covariance - 2.0/np.sqrt(dim) * covariance_2)\n",
    "    wasserstein_distance = math.sqrt(part1 + 1 + part2)\n",
    "    return -wasserstein_distance \n",
    "\n",
    "\n",
    "def mean_distance_of_true_pairs(features_modality1: torch.Tensor, features_modality2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean cosine similarity of true pairs between two modalities.\n",
    "\n",
    "    Args:\n",
    "        features_modality1 (torch.Tensor): Normalized feature matrix of modality 1 with shape [N, D].\n",
    "        features_modality2 (torch.Tensor): Normalized feature matrix of modality 2 with shape [N, D].\n",
    "\n",
    "    Returns:\n",
    "        float: Mean cosine similarity of true pairs.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = torch.matmul(features_modality1, features_modality2.T)\n",
    "\n",
    "    # Extract diagonal elements (true pairs)\n",
    "    cosine_sim_diag = torch.diag(cosine_sim)\n",
    "\n",
    "    # Compute mean cosine similarity of true pairs\n",
    "    cosine_tv_mean = torch.mean(cosine_sim_diag).item()\n",
    "\n",
    "    return cosine_tv_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: torch.nn.Module, test_loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics, along with additional metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing all evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare storage for embeddings\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "\n",
    "    # IDs for retrieval\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "\n",
    "    current_index = 0\n",
    "    \n",
    "    tokenizer = open_clip.get_tokenizer('RN50')\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move embeddings to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds = text_embeds.cpu()\n",
    "\n",
    "            # Store embeddings\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # Assign unique IDs\n",
    "            bs = images.size(0)\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # Shape: [N, D]\n",
    "    all_text_embeds = torch.cat(all_text_embeds, dim=0)    # Shape: [N, D]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval and metric computations\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Compute retrieval metrics\n",
    "    log_forward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')   # Text-to-Vision\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward') # Vision-to-Text\n",
    "\n",
    "    # Compute additional metrics\n",
    "    gap = compute_gap(all_image_embeds, all_text_embeds)\n",
    "    mean_ang_image = compute_mean_angular_value_of_a_modality(all_image_embeds)\n",
    "    mean_ang_text = compute_mean_angular_value_of_a_modality(all_text_embeds)\n",
    "    uniformity_metric = uniformity(all_image_embeds, all_text_embeds)\n",
    "    mean_cos_true_pairs = mean_distance_of_true_pairs(all_image_embeds, all_text_embeds)\n",
    "\n",
    "    # Combine all metrics into final_log\n",
    "    final_log = {\n",
    "        **log_forward,\n",
    "        **log_backward,\n",
    "        'gap': round(gap, 4),\n",
    "        'mean_angular_value_image': round(mean_ang_image, 4), # round to 4 decimal places\n",
    "        'mean_angular_value_text': round(mean_ang_text, 4),\n",
    "        'uniformity': round(uniformity_metric, 4),\n",
    "        'mean_cosine_similarity_true_pairs': round(mean_cos_true_pairs, 4)\n",
    "    }\n",
    "\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "    print()\n",
    "    \n",
    "    wandb.log(final_log)\n",
    "\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, test_loader, device):\n",
    "\n",
    "    # Create model & transforms from scratch (no pretrained weights) #TODO: Use the tokenizer from the chosen model, not the default one\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        config[\"model\"],\n",
    "        pretrained=None,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    tokenizer = open_clip.get_tokenizer(config[\"model\"])\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # If you want to fine-tune *everything* from scratch, ensure all parameters require grad:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Set up training parameters from the config\n",
    "    lr = config[\"learning_rate\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "\n",
    "    # Move the model to multiple GPUs\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])  # Use 4 GPUs\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    current_batch = 0\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, captions_list in tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\"):\n",
    "            \n",
    "            current_batch += 1\n",
    "            \n",
    "            # Move data to the primary device\n",
    "            images = images.to(device)\n",
    "            captions = captions_list\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer(captions)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Encode image and text\n",
    "            image_embeds = model.module.encode_image(images)  # Use .module for methods inside DataParallel\n",
    "            text_embeds = model.module.encode_text(text_tokens)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "            text_embeds  = F.normalize(text_embeds, dim=-1)\n",
    "            \n",
    "            # Compute loss based on the experiment type\n",
    "            if config[\"loss_type\"] == \"anchor\":\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "            elif config[\"loss_type\"] == \"anchor+lunif\":\n",
    "                lunif_img = lunif_loss(image_embeds)\n",
    "                lunif_txt = lunif_loss(text_embeds)\n",
    "                lunif = (lunif_img + lunif_txt) / 2\n",
    "                loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature) + lunif\n",
    "            elif config[\"loss_type\"] == \"lunif_n_iters+frozen(text_embed)\":\n",
    "                if current_batch <= config[\"lunif_n_iters\"]:\n",
    "                    lunif_img = lunif_loss(image_embeds)\n",
    "                    lunif_txt = lunif_loss(text_embeds)\n",
    "                    lunif = (lunif_img + lunif_txt) / 2\n",
    "                    loss = lunif\n",
    "                else: # train on anchor loss with frozen text embeddings\n",
    "                    text_embeds = text_embeds.detach()\n",
    "                    loss = contrastive_loss(image_embeds, text_embeds, temperature=temperature)\n",
    "                    \n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\"if current_batch % config[\"visualize_every_n_batches\"] == 0 and config[\"visualize_every_n_batches\"] != False:\n",
    "                visualize_embeddings(text_embeds, \n",
    "                                    image_embeds, \n",
    "                                    sample_size=1000, \n",
    "                                    method='umap', \n",
    "                                    title=\"CLIP Embeddings Visualization\",\n",
    "                                    save_path=\"embeddings_plot.png\")\n",
    "                \n",
    "                visualize_embeddings(text_embeds, \n",
    "                                    image_embeds, \n",
    "                                    sample_size=1000, \n",
    "                                    method='umap', \n",
    "                                    title=\"CLIP Embeddings Visualization\",\n",
    "                                    save_path=\"embeddings_plot.png\")\n",
    "            \n",
    "            if current_batch % config[\"evaluate_every_n_batches\"] == 0 and config[\"evaluate_every_n_batches\"] != False:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}]  Batch: {current_batch}  Loss: {loss.item():.5f}\")\n",
    "                evaluate_model(model, test_loader, device)\"\"\"\n",
    "\n",
    "        \n",
    "        #print(f\"[Epoch {epoch+1}/{epochs}]  Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if config[\"evaluate_and_visualize_every_epoch\"] == True:\n",
    "            evaluate_model(model, test_loader, device)\n",
    "            \n",
    "            visualize_embeddings(text_embeds, \n",
    "                                image_embeds, \n",
    "                                sample_size=1000, \n",
    "                                method='umap', \n",
    "                                title=\"CLIP Embeddings Visualization\",\n",
    "                                save_path=\"embeddings_plot_umap.png\")\n",
    "            visualize_embeddings(text_embeds, \n",
    "                                image_embeds, \n",
    "                                sample_size=1000, \n",
    "                                method='tsne',\n",
    "                                title=\"CLIP Embeddings Visualization\",\n",
    "                                save_path=\"embeddings_plot_tsne.png\")\n",
    "        \n",
    "        if config[\"save_checkpoint_every_n_epochs\"] % (epoch+1) == 0:\n",
    "            torch.save(model.state_dict(), f\"models/model_\" + config[\"run_name\"] + f\"_epoch_{epoch+1}.pt\")\n",
    "            print(f\"Model saved at epoch {epoch+1}\")\n",
    "        \n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(config):\n",
    "\n",
    "    # Path to train images and annotations\n",
    "    train_image_dir = './data/coco/images/train2017/'                          # Path to train2017 images\n",
    "    train_annotation_file = './data/coco/annotations/captions_train2017.json'  # Path to train2017 captions\n",
    "\n",
    "    # Path to test (val) images and annotations\n",
    "    test_image_dir = './data/coco/images/val2017/'                          # Path to val2017 images\n",
    "    test_annotation_file = './data/coco/annotations/captions_val2017.json'  # Path to val2017 captions\n",
    "\n",
    "    # Define the transform to be applied to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to the model's required input size\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_coco = dset.CocoCaptions(\n",
    "        root=train_image_dir,\n",
    "        annFile=train_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_coco = dset.CocoCaptions(\n",
    "        root=test_image_dir,\n",
    "        annFile=test_annotation_file,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    if config[\"num_train_samples\"] != -1:\n",
    "        print(f\"Subsetting the training dataset to {config['num_train_samples']} samples\")\n",
    "        # Subset the training dataset\n",
    "        num_training_samples = config[\"num_train_samples\"]\n",
    "        subset_indices = list(range(num_training_samples))\n",
    "        train_coco = Subset(train_coco, subset_indices)\n",
    "    \n",
    "    if config[\"num_test_samples\"] != -1:\n",
    "        print(f\"Subsetting the test dataset to {config['num_test_samples']} samples\")\n",
    "        # Subset the test dataset\n",
    "        num_test_samples = config[\"num_test_samples\"]\n",
    "        subset_indices = list(range(num_test_samples))\n",
    "        test_coco = Subset(test_coco, subset_indices)\n",
    "\n",
    "    # Every image has 5 captions at max, we need to sample one of them\n",
    "    # Create collate function to sample one caption per image\n",
    "    def collate_fn(batch):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        sel_captions = []\n",
    "        for list_captions in captions:\n",
    "            caption = random.choice(list_captions)\n",
    "            sel_captions.append(caption)\n",
    "        return images, sel_captions\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_coco, batch_size=batch_size, shuffle=True , drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    test_loader  = DataLoader(test_coco , batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, num_workers=12)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # NumPy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU random numbers\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU random numbers for a single GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random numbers for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark for deterministic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # Finish any existing W&B runs before starting a new one\n",
    "    wandb.finish()\n",
    "\n",
    "    # Initialize your W&B run\n",
    "    wandb.init(project=\"sparsify-clip\", config=config, name=config[\"run_name\"])\n",
    "    \n",
    "    # Print the config\n",
    "    print(\"Config:\", config)\n",
    "    \n",
    "    # Set the device\n",
    "    device_id = config[\"device_id\"]\n",
    "    device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"\\nLoading the dataset...\")\n",
    "    train_loader, test_loader = dataset_loader(config)\n",
    "    print(\"Dataset loaded.\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(config, train_loader, test_loader, device)\n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    print(\"Final evaluation of the model...\")\n",
    "    final_log = evaluate_model(model, test_loader, device)\n",
    "    print(\"Evaluation complete.\\n\")\n",
    "    \n",
    "    # Save the model and upload it to W&B\n",
    "    torch.save(model.state_dict(), config[\"run_name\"] + \".pt\")\n",
    "    wandb.save(config[\"run_name\"] + \".pt\")    \n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoostale\u001b[0m (\u001b[33mnoostale-organization\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tesista2/sparsify-clip/wandb/run-20250103_231200-h2hjqvis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/h2hjqvis' target=\"_blank\">RN50_anchor_2025-01-03-23-12-00</a></strong> to <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/sparsify-clip' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/sparsify-clip/runs/h2hjqvis' target=\"_blank\">https://wandb.ai/noostale-organization/sparsify-clip/runs/h2hjqvis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'run_name': 'RN50_anchor_2025-01-03-23-12-00', 'device_id': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 100, 'model': 'RN50', 'temperature': 0.07, 'loss_type': 'anchor', 'lunif_n_iters': 300, 'save_checkpoint_every_n_epochs': 10, 'evaluate_and_visualize_every_epoch': True, 'num_train_samples': -1, 'num_test_samples': -1, 'evaluate_every_n_batches': -1, 'visualize_every_n_batches': -1}\n",
      "\n",
      "Loading the dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded.\n",
      "\n",
      "Training the model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleTokenizer' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Baseline model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Anchor + Lunif (HAVE TO FINISH TESTING)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchor+lunif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Final evaluation of the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(config, train_loader, test_loader, device):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Create model & transforms from scratch (no pretrained weights) #TODO: Use the tokenizer from the chosen model, not the default one\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     model, _, preprocess \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\n\u001b[1;32m      5\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m         pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Put the model into training mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleTokenizer' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"run_name\":                     \"{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),  # A readable name for this run\n",
    "    \"device_id\":                    1,      # GPU id\n",
    "    \"seed\":                         42,     # Random seed\n",
    "    \n",
    "    \"learning_rate\":                1e-4,\n",
    "    \"batch_size\":                   256,\n",
    "    \"epochs\":                       100,\n",
    "    \"model\":                        \"RN50\",\n",
    "    \n",
    "    \"temperature\":                  0.07,\n",
    "    \n",
    "    \"loss_type\":                    \"anchor+lunif\",   # anchor, anchor+lunif\n",
    "    \"lunif_n_iters\":                300,\n",
    "    \n",
    "    \n",
    "    \"save_checkpoint_every_n_epochs\": 10,\n",
    "    \"evaluate_and_visualize_every_epoch\": True,\n",
    "    \n",
    "    \"num_train_samples\":            -1,            # -1 for all\n",
    "    \"num_test_samples\":             -1,            # -1 for all\n",
    "    \"evaluate_every_n_batches\":     -1,            # -1 for no\n",
    "    \"visualize_every_n_batches\":    -1,            # -1 for no\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Baseline\n",
    "    config[\"loss_type\"] = \"anchor\"\n",
    "    config[\"run_name\"] = config[\"model\"] + \"_\" + config[\"loss_type\"] + \"_\" + config[\"run_name\"] \n",
    "    print(\"\\nTraining Baseline model\")\n",
    "    main(config)\n",
    "    \n",
    "    \n",
    "    # Anchor + Lunif (HAVE TO FINISH TESTING)\n",
    "    config[\"loss_type\"] = \"anchor+lunif\"\n",
    "    config[\"run_name\"] = config[\"model\"] + \"_\" + config[\"loss_type\"] + \"_\" + config[\"run_name\"] \n",
    "    print(\"\\nTraining Anchor + Lunif model\")\n",
    "    main(config)\n",
    "    \n",
    "    \n",
    "    # Lunif(50itr)+frozen(text_embed)\n",
    "    config[\"loss_type\"] = \"lunif_n_iters+frozen(text_embed)\"\n",
    "    config[\"run_name\"] = config[\"model\"] + \"_\" + config[\"loss_type\"] + \"_\" + config[\"run_name\"] \n",
    "    print(\"\\nTraining lunif_n_iters+frozen(text_embed) model\")\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def evaluate_model(model, test_loader, device):\\n    \\'\\'\\'\\n    Evaluate the (OpenCLIP) model on the given test_loader by computing\\n    text-to-image and image-to-text retrieval metrics.\\n\\n    Args:\\n        model (nn.Module): The trained (DataParallel) model.\\n        test_loader (DataLoader): A DataLoader for the evaluation set.\\n        device (torch.device): The device (CPU or GPU).\\n    \\'\\'\\'\\n    \\n    # Put model into eval mode\\n    model.eval()\\n    \\n    # Prepare storage\\n    all_image_embeds = []\\n    all_text_embeds  = []\\n    \\n    # IDs for retrieval\\n    # We\\'ll assign each sample a unique ID. Because your `collate_fn` is\\n    # picking exactly one caption per image, we can treat each batch entry\\n    # as a 1:1 mapping of (image_i <-> text_i).\\n    ids_img = []\\n    ids_txt = []\\n    \\n    current_index = 0\\n\\n    # No gradient needed during evaluation\\n    with torch.no_grad():\\n        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\\n            # Move images to device\\n            images = images.to(device)\\n\\n            # Tokenize captions\\n            text_tokens = tokenizer.tokenize(captions_list)\\n            text_tokens = text_tokens.to(device)\\n\\n            # Extract embeddings using the .module references in DataParallel\\n            image_embeds = model.module.encode_image(images)\\n            text_embeds  = model.module.encode_text(text_tokens)\\n\\n            # Move them to CPU for later concatenation\\n            image_embeds = image_embeds.cpu()\\n            text_embeds  = text_embeds.cpu()\\n            \\n            # Track\\n            bs = images.size(0)\\n            all_image_embeds.append(image_embeds)\\n            all_text_embeds.append(text_embeds)\\n\\n            # For retrieval, we label these samples from current_index to current_index + bs - 1\\n            sample_ids = list(range(current_index, current_index + bs))\\n            ids_img.extend(sample_ids)\\n            ids_txt.extend(sample_ids)\\n            current_index += bs\\n    \\n    # Concatenate everything\\n    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\\n    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\\n\\n    # Normalize embeddings for more stable retrieval\\n    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\\n    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\\n\\n    # Compute pairwise similarity: [N_text, N_image]\\n    # Because we aligned IDs, this is effectively [N, N].\\n    similarity_matrix = all_text_embeds @ all_image_embeds.t()\\n\\n    # Use the given function compute_metric_ret to compute retrieval metrics.\\n    # text->image: direction=\\'forward\\'\\n    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction=\\'forward\\')\\n    # image->text: direction=\\'backward\\'\\n    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction=\\'backward\\')\\n\\n    # You can combine or print them:\\n    final_log = {**log_forward, **log_backward}\\n    print(\"Evaluation Results:\", final_log)\\n\\n    return final_log'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %doctest_mode\n",
    "\n",
    "\n",
    "def dataset_details():\n",
    "    # Print dataset details\n",
    "    print('Number of samples:', len(train_coco)) # 118287 images\n",
    "\n",
    "    # Access a specific sample (4th sample here)\n",
    "    img, target = train_coco[3]  # Load the 4th sample (index 3)\n",
    "\n",
    "    # Display information about the sample\n",
    "    print(\"Image Size:\", img.size())  # Torch tensor size\n",
    "    #plt.imshow(img.permute(1, 2, 0))  # Display the image\n",
    "    print(\"Captions:\", target)  # Captions for the image\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    # images.shape is e.g. (N, 3, 224, 224)\n",
    "    # captions_list has length N, but each item might be a tuple of possible captions\n",
    "\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Image batch size:\", images.shape[0], \"Shape:\", images.shape)\n",
    "    print(\"Captions list length:\", len(captions_list))\n",
    "    \n",
    "    print(\"Captions list:\", list(captions_list))\n",
    "\n",
    "    print(\"Number of chosen captions:\", len(list(captions_list[0])))\n",
    "    \n",
    "    captions = list(captions_list[0])\n",
    "\n",
    "    # Then tokenize\n",
    "    text_tokens = tokenizer.tokenize(captions)\n",
    "    print(\"Text tokens shape:\", text_tokens.shape)\n",
    "\n",
    "    # Now encode\n",
    "    #image_embeds = model.encode_image(images.to(device))\n",
    "    #text_embeds = model.encode_text(text_tokens.to(device))\n",
    "\n",
    "    # Should both be shape (N, D)\n",
    "    #print(\"Image embeds shape:\", image_embeds.shape)\n",
    "    #print(\"Text  embeds shape:\", text_embeds.shape)\n",
    "\n",
    "    break  # just to test one batch\n",
    "    \n",
    "\n",
    "def collate_fn_debug(batch):\n",
    "    print(\"Bath type:\", type(batch)) # This is a list\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Batch:\", batch)\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    print(\"Images type:\", type(images))\n",
    "    print(\"Images size:\", len(images))\n",
    "    print(\"Images:\", images)\n",
    "    \n",
    "    print(\"Captions type:\", type(captions))\n",
    "    print(\"Captions size:\", len(captions))\n",
    "    print(\"Captions:\", captions) # This is a tuple of lists, each list contains 5 captions for each image\n",
    "    \n",
    "    # Select one caption per image\n",
    "    sel_captions = []\n",
    "    for list_captions in captions:\n",
    "        #print(\"List Captions:\", list_captions)\n",
    "        caption = random.choice(list_captions)\n",
    "        sel_captions.append(caption)\n",
    "    \n",
    "    print(\"Selected Captions:\", sel_captions)    \n",
    "\n",
    "\n",
    "\n",
    "for images, captions_list in train_loader:\n",
    "    break\n",
    "\n",
    "# DONE: ensure that each tuple of captions has the same length, or the data loader will fail (defalut is collate(samples, collate_fn_map=collate_fn_map) from error message)\n",
    "\n",
    " \"\"\"\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\"\"\"def compute_metric_ret(score_matrix, ids, ids_txt, direction='forward'):\n",
    "    \n",
    "    # Check that the score matrix has the correct shape\n",
    "    assert score_matrix.shape == (len(ids_txt),len(ids))\n",
    "\n",
    "    if direction == 'forward': ### text-to-vision retrieval\n",
    "        indice_matrix = score_matrix.sort(dim=-1,descending=True)[1].tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids_txt)):\n",
    "            # gt_indice = ids.index(ids_txt[i][0])\n",
    "            gt_indice = ids.index(ids_txt[i])\n",
    "            rank.append(indice_matrix[i].index(gt_indice))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        vr_r1 = (rank < 1).sum().item() / len(ids_txt)\n",
    "        vr_r5 = (rank < 5).sum().item() / len(ids_txt)\n",
    "        vr_r10 = (rank < 10).sum().item() / len(ids_txt)\n",
    "        v_medianR = torch.median(rank).item() +1\n",
    "        v_meanR = torch.mean(rank).item() +1\n",
    " \n",
    "        eval_log = {'forward_r1': round(vr_r1*100,3),\n",
    "                    'forward_recall': f'{round(vr_r1*100,1)}/{round(vr_r5*100,1)}/{round(vr_r10*100,3)}',\n",
    "                    'forward_ravg': round((vr_r1 + vr_r5 + vr_r10)/3 *100,3)\n",
    "                   }\n",
    "   \n",
    "    else: ### vision-to-text retrieval\n",
    "       \n",
    "        indice_matrix = score_matrix.sort(dim=0,descending=True)[1].permute(1,0).tolist()\n",
    "        rank = []\n",
    "        for i in range(len(ids)):\n",
    "            gt_indices=[]\n",
    "            for idx, id in enumerate(ids_txt):\n",
    "                if id == ids[i]:\n",
    "                    gt_indices.append(idx)\n",
    "\n",
    "            rank.append(min([indice_matrix[i].index(idx) for idx in gt_indices]))\n",
    "        \n",
    "        rank = torch.tensor(rank).to(score_matrix)\n",
    "        \n",
    "        tr_r1 = (rank < 1).sum().item() / len(ids)\n",
    "        tr_r5 = (rank < 5).sum().item() / len(ids)\n",
    "        tr_r10 = (rank < 10).sum().item() / len(ids)\n",
    "        t_medianR = torch.median(rank).item() +1\n",
    "        t_meanR = torch.mean(rank).item() +1\n",
    "\n",
    "        eval_log = {\n",
    "                    'backward_r1': round(tr_r1*100,3),\n",
    "                    'backward_recall': f'{round(tr_r1*100,1)}/{round(tr_r5*100,1)}/{round(tr_r10*100,3)}',\n",
    "                    'backward_ravg': round((tr_r1 + tr_r5 + tr_r10)/3 *100,3)\n",
    "                  }\n",
    "    \n",
    "    return eval_log\"\"\"\n",
    "\n",
    "\"\"\"def evaluate_model(model, test_loader, device):\n",
    "    '''\n",
    "    Evaluate the (OpenCLIP) model on the given test_loader by computing\n",
    "    text-to-image and image-to-text retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained (DataParallel) model.\n",
    "        test_loader (DataLoader): A DataLoader for the evaluation set.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    '''\n",
    "    \n",
    "    # Put model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare storage\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds  = []\n",
    "    \n",
    "    # IDs for retrieval\n",
    "    # We'll assign each sample a unique ID. Because your `collate_fn` is\n",
    "    # picking exactly one caption per image, we can treat each batch entry\n",
    "    # as a 1:1 mapping of (image_i <-> text_i).\n",
    "    ids_img = []\n",
    "    ids_txt = []\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # No gradient needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, captions_list in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Tokenize captions\n",
    "            text_tokens = tokenizer.tokenize(captions_list)\n",
    "            text_tokens = text_tokens.to(device)\n",
    "\n",
    "            # Extract embeddings using the .module references in DataParallel\n",
    "            image_embeds = model.module.encode_image(images)\n",
    "            text_embeds  = model.module.encode_text(text_tokens)\n",
    "\n",
    "            # Move them to CPU for later concatenation\n",
    "            image_embeds = image_embeds.cpu()\n",
    "            text_embeds  = text_embeds.cpu()\n",
    "            \n",
    "            # Track\n",
    "            bs = images.size(0)\n",
    "            all_image_embeds.append(image_embeds)\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "            # For retrieval, we label these samples from current_index to current_index + bs - 1\n",
    "            sample_ids = list(range(current_index, current_index + bs))\n",
    "            ids_img.extend(sample_ids)\n",
    "            ids_txt.extend(sample_ids)\n",
    "            current_index += bs\n",
    "    \n",
    "    # Concatenate everything\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape [N, embed_dim]\n",
    "    all_text_embeds  = torch.cat(all_text_embeds, dim=0)   # shape [N, embed_dim]\n",
    "\n",
    "    # Normalize embeddings for more stable retrieval\n",
    "    all_image_embeds = F.normalize(all_image_embeds, dim=-1)\n",
    "    all_text_embeds  = F.normalize(all_text_embeds, dim=-1)\n",
    "\n",
    "    # Compute pairwise similarity: [N_text, N_image]\n",
    "    # Because we aligned IDs, this is effectively [N, N].\n",
    "    similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "\n",
    "    # Use the given function compute_metric_ret to compute retrieval metrics.\n",
    "    # text->image: direction='forward'\n",
    "    log_forward  = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='forward')\n",
    "    # image->text: direction='backward'\n",
    "    log_backward = compute_metric_ret(similarity_matrix, ids_img, ids_txt, direction='backward')\n",
    "\n",
    "    # You can combine or print them:\n",
    "    final_log = {**log_forward, **log_backward}\n",
    "    print(\"Evaluation Results:\", final_log)\n",
    "\n",
    "    return final_log\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsify_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
